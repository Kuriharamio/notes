# LM算法

## 一阶导数与梯度

### 一阶导数

对于一元函数 $y = f(x)$，其一阶导数为：

$$
f'(x) = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}
$$

表示函数在该点的变化率。

### 梯度

对于多元函数 $f(x_1, x_2, \ldots, x_n)$，其梯度是一个向量，表示函数在各个变量方向上的偏导数，记作 $\nabla f$，即：

$$
\nabla f = \begin{bmatrix}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_n}
\end{bmatrix}
$$

梯度的方向是函数在该点增长最快的方向，其大小表示变化率的大小。



## 二阶导数与 Hessian 矩阵

### 二阶导数

对于一元函数 $y = f(x)$，其二阶导数为：

$$
f''(x) = \frac{d}{dx} \left( f'(x) \right )
$$

表示函数一阶导数的变化率。

### Hessian 矩阵

对于多元函数 $f(x_1, x_2, \ldots, x_n)$，其 Hessian 矩阵是一个 $n \times n$ 的矩阵，表示函数在各个变量之间的二阶偏导数，记作 H，即：

$$
H = \begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
$$

Hessian 矩阵在优化问题中非常重要，它描述了函数的局部曲率信息。



## LM算法在相机标定中的应用

  1. **单目标定或双目标定中内参固定计算最佳外参** ：在 OpenCV 中对应函数为 `findExtrinsicCameraParams2`。
  2. **单目标定中内外参都不固定计算最佳内外参** ：在 OpenCV 中对应函数为 `calibrateCamera2`。
  3. **双目标定中左右相机内外参及位姿都不固定计算最佳参数** ：在 OpenCV 中对应函数为 `stereoCalibrate`。



## 优化算法初探

  1. **目标函数与最小二乘问题** ：一般待优化的目标函数由误差平方和组成，即最小二乘问题。可通过令目标函数导数（可导时）为零求解，常用方法是迭代求解。
  2. **迭代求解过程** ：
     * 给定初值 θ₀，对应目标函数值 f(θ₀)，初值可经验估计或随机指定（随机指定收敛慢）。
     * 改变 θ₀ 值得到 θ₁，若 f(θ₁) < f(θ₀)，则迭代方向使目标函数值减小。
     * 循环迭代至满足终止条件（如相邻几次迭代目标函数值差别在阈值范围内或达到最大迭代次数）。

  3. **优化方法** ：确定增量 △x 的方法有梯度下降、牛顿、高斯牛顿、LM 等。很多情况下找到的 “全局最小值” 实际是局部最小值，若目标函数是凸函数，则局部最小值即为全局最小值。



## 凸函数概念及相关判断

  1. **凸函数定义** ：若一个函数是凸函数，则它只有一个极小值点，也就是最小值点。
  2. **凸函数判断** ：
     * 一元二次可微函数，二阶导数正则为严格凸函数。
     * 多元二次可微函数，Hessian 矩阵正定则为严格凸函数。
     * 非负凸函数和仍为凸函数。

  3. **Hessian 矩阵与正定矩阵** ：
     * Hessian 矩阵：实数函数 f(x₁,...,xn)，所有二阶偏导数存在时的 n×n 方阵。
     * 正定矩阵判断：所有特征值均为正数，或各阶主子式均大于零。



## 梯度下降法

  1. **基本原理** ：求解无约束优化问题的简单古老方法，虽实用性低，但许多有效算法基于它改进。本质是一阶优化算法，也叫最速下降法，对应梯度上升法。对于函数 f(x,y)，在点 (x₀,y₀)，梯度向量方向是 (∂f/∂x₀, ∂f/∂y₀)，即 f(x,y) 增加最快方向。
  2. **迭代更新方式** ：以一元函数为例，目标函数为 F(x)，梯度符号为 ∇，自变量 x 每次按特定方式迭代更新。
  3. **问题** ：初始值选取和步长选取是梯度下降法的两个关键问题。



## 算法演进

  1. **牛顿法** ：
     * **原理** ：梯度下降法在山谷中易出现锯齿状迭代，牛顿法可避免。基于泰勒展开通式，目标函数在 x 处进行二阶泰勒展开，表达式为：
       $$
       f(x^{(k)} + \Delta x^{(k)}) \approx f(x^{(k)}) + J(x^{(k)}) \Delta x + \frac{1}{2} \Delta x^T H(x^{(k)}) \Delta x
       $$
       其中，$J(x^{(k)})$ 是 $f(x^{(k)})$ 关于 x 的 Jacobian 矩阵（一阶导数），$H(x^{(k)})$ 是 Hessian 矩阵（二阶导数）。对 $\Delta x$ 求导并令其为 0，得到步长：
       $$
       \Delta x^{(k)} = -H(x^{(k)})^{-1} J(x^{(k)})^T
       $$
     
     * **优势** ：利用梯度和梯度变化速度（二阶导数）信息，但计算量消耗大。
     
  2. **高斯牛顿法** ：
     
     * **原理** ：用雅克比矩阵乘积近似代替牛顿法中的二阶 Hessian 矩阵。对 $f(x^{(k)} + \Delta x^{(k)})$ 进行一阶泰勒展开，转化为最小二乘问题，得到：
       $$
       \Delta x^{(k)} = - (J(x^{(k)})^T J(x^{(k)}) )^{-1} J(x^{(k)})^T f(x^{(k)})
       $$
       但近似 Hessian 矩阵 $J(x^{(k)})^T J(x^{(k)})$ 可能奇异或病态，且泰勒展开仅在较小范围近似。
       
     * **问题** ：高斯牛顿法采用近似二阶泰勒函数仅在展开点附近有较好近似效果，步长太大时近似不准确，需给步长加信赖区域。
     
  3. **Levenberg-Marquardt（LM）法** ：
     
     * **原理** ：衰减最小二乘法，更加鲁棒，牺牲一定收敛速度。参数 u 影响迭代方向和步长大小，u 取值较大时 uI 占主要地位，u 取值较小时 H 占主要地位。LM 法通过调整参数 u 来平衡 Gauss-Newton 法和梯度下降法的特点，具体公式为：
       $$
       \Delta x^{(k)} = - (H(x^{(k)}) + u^{(k)} I)^{-1} J(x^{(k)})^T f(x^{(k)})
       $$
       
       其中，I 是单位矩阵，$u^{(k)}$ 是非负数。
       
     * **步骤** ：
       1. 给定初始点 $x^{(0)}$，计算初始信赖域半径 $u^{(0)}$。
       2. 对于第 k 次迭代，求出步长 $\Delta x^{(k)}$，并计算得到 $\rho^{(k)}$，根据 $\rho^{(k)}$ 接近 1 的程度调整信赖区域范围：
          * 如果 $\rho^{(k)}$ 接近 1，认为近似比较准确，可以适当增大信赖区域，即减小 $u^{(k+1)}$。
          * 如果 $\rho^{(k)}$ 远小于 1，需要缩小信赖范围，即增大 $u^{(k+1)}$。
     
     * **优势** ：LM 算法可一定程度避免系数矩阵的非奇异和病态问题，提供更鲁棒、准确的步长，在相机标定、视觉 SLAM 等领域应用广泛。