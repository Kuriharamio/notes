# Overview

## 监督学习(Supervised learning)

- learning from data labeled with the right answers

### Regression（回归）：

- predict a number infinitely from many possible numbers

- x->y 

- input -> output label

  <img src="D:\Study\Note\机器学习\photos\image-20240914005110233.png" alt="image-20240914005110233" style="zoom:50%;" />

- 给算法提供数据集，使其产生更多的数据预测



### Classicication（分类）：

- predict categories, small number of possible outputs
  - 0 1
  - cat dog
  - benign malignant
- classes（输出类）= category（输出类别）



## 无监督学习(Unsupervised learning)

- find something interesting in unlabeled data
- takes data without labels and tries to automatically group them into clusters

### Clustering algorithm（聚类算法）：

- 决定将未标记的数据分配给多个不同的集群
  - eg. 
    - google news 
      - 将类似的新闻文章放在一起
    - DNA microarray 
      - 分析不同类型DNA、不同类型人
    - user database
      - 分析用户集群情况，以更好地提供服务



### Anomaly detection（异常检测）：

- 用于检测异常事件

 

### Dimensionality reduction（数据降维）：

- compress data using fewer numbers and lose as little imformation as possible



# Linear Regression Model

- eg.
  - <img src="D:\Study\Note\机器学习\photos\image-20240914133023250.png" alt="image-20240914133023250" style="zoom:50%;" />



- 概念

  - training set 
  - x = input variable / feature / inut feature
  - y = output variable / target variable
  - m = number of training examples
  - $(x^{(i)},y^{(i)})$ = $i^{th}$ training example
  - $\hat y$ = prediction or estimate of target

  <img src="D:\Study\Note\机器学习\photos\image-20240914133900819.png" alt="image-20240914133900819" style="zoom: 33%;" />

- represent $f$ :

  - $f_{w,b}(x)=wx+b$
  - w,b : parameters coefficients



## Cost Function

<img src="D:\Study\Note\机器学习\photos\image-20240914140207756.png" alt="image-20240914140207756" style="zoom:50%;" />

- model
  $$
  f_{w,b}(x)=wx+b
  $$
  
- parameters
  $$
  w,b
  $$
  
- cost function
  $$
  J(w,b)=\frac1{2m}\sum^m_{i=1}(f_{w,b}(x^{(i)})-y^{(i)})^2
  $$
  
- goal
  $$
  \min_{w,b}J(w,b)
  $$

<img src="D:\Study\Note\机器学习\photos\image-20240914144141084.png" alt="image-20240914144141084" style="zoom:50%;" />





## Gradient descent

- outline
  - start with some w,b (set w=b=0)
  - keep changing w,b, to reduce J(w,b)
  - until we settle at or near a minimum

<img src="D:\Study\Note\机器学习\photos\image-20240914150253001.png" alt="image-20240914150253001" style="zoom:50%;" />

### algorithm

$$
w=w-\alpha\frac{\partial }{\partial w}J(w,b)\\
b=b-\alpha\frac{\partial }{\partial b}J(w,b)\\
$$

- $\alpha $ ：learning rate
  - 太小：收敛速度慢
  - 太大：可能在收敛点来回摆动，甚至发散
  - 当接近 local minima 时，偏导变小，更新的步骤也自动变小
- update w,b simultaneously
  - batch gradient descent
  - 在梯度下降的每一步中，我们都在查看所有的训练示例
- local minima & global minima



## Multiple Features（多元）

### 概念

$$
f_{w,b}(x)=b+\sum_{i=1}^nw_ix_i
$$

- $x_j$ = $j^{th}$ feature
- n = number of feature
- $\vec x^{(i)}$ = freatures of $i^{th}$ training example
- $x_j^{(i)}$ = value of feature j in $i^{th}$ training example

- 用vector重写：
  $$
  \vec w=[w_1,w_2,...,w_n]\\
  \vec x=[x_1,x_2,...,x_n]\\
  f_{w,b}(x)=\vec w ·\vec x +b
  $$

- 称为多元线性回归 multiple linear regression



### Vectorization（向量化、矢量化）

使用numpy实现矩阵运算可以提效（并行处理，而非for循环）

```python
w = np.array([1.0,2.5,-3.3])
b = 4
x = np.array([10,20,30])
#点积运算
f=np.dot(w,x)+b
```



#### Gradient descent in multiple variable linear regression

$$
w_j=w_j-\alpha\frac{\part}{\part w_j}J(\vec w,b)\\
b=b-\alpha\frac{\part}{\part b}J(\vec w,b)\\
$$

## Feature Scaling(特性伸缩)

使梯度下降更快

- 特征值较大时，w可能较小，反之可能更大

- 缩放数据尺度

<img src="D:\Study\Note\机器学习\photos\image-20240914173241654.png" alt="image-20240914173241654" style="zoom:50%;" />

- 当我们有不同的 features，它们的value具有非常不同的范围，会导致梯度下降运行缓慢
  - <img src="D:\Study\Note\机器学习\photos\image-20240914173958350.png" alt="image-20240914173958350" style="zoom:50%;" />
- 若将他们缩放到相似的scale，具有可比的值范围，可以显著加快梯度下降

- 方法
  - <img src="D:\Study\Note\机器学习\photos\image-20240914173535256.png" alt="image-20240914173535256" style="zoom:50%;" />
  - <img src="D:\Study\Note\机器学习\photos\image-20240914173649259.png" alt="image-20240914173649259" style="zoom:50%;" />
  - <img src="D:\Study\Note\机器学习\photos\image-20240914173812946.png" alt="image-20240914173812946" style="zoom:50%;" />
  - 



## Checking Gradient Descent for Convergence

识别梯度下降时是否找到 local minima

- 绘图判断 learning curve
  - <img src="D:\Study\Note\机器学习\photos\image-20240914174234036.png" alt="image-20240914174234036" style="zoom:50%;" />
  - 如果曲线上升，要不就是alpha太大，要不就是代码有bug
- Automatic convergence test
  - 单次decrease小于 $\epsilon$ 



### Choosing the Learning Rate

- up and down 或者 一直up

  <img src="D:\Study\Note\机器学习\photos\image-20240914174656042.png" alt="image-20240914174656042" style="zoom:50%;" />

  - rate 太大了
  - 代码写成了 +成本

- 先设置很小的alpha

  - 下降了，再调整alpha
  - 上升了，代码有bug

- 调整了alpha

  - 找到上下两界
  - 再从中间选择最佳的alpha

- values of alpha

  - 0.001
  - x3 = 0.003
  - x3 = 0.01



### Feature Engineering

选择最好的、符合实际的、显著的特征，而非一直堆叠导致效率降低

<img src="D:\Study\Note\机器学习\photos\image-20240914175155284.png" alt="image-20240914175155284" style="zoom: 33%;" />

### Polynomial Regression（多项式回归）

使曲线更符合实际

<img src="D:\Study\Note\机器学习\photos\image-20240914175445596.png" alt="image-20240914175445596" style="zoom:50%;" />

- 只有二次的话最终会下降，不符合实际，故选择三次方
- 要记得 feature scaling





# Classification

- binary classification： 2 values（0、1）（true、false）

  <img src="D:\Study\Note\机器学习\photos\image-20240915023830117.png" alt="image-20240915023830117" style="zoom:50%;" />

## Motivation

- 线性回归不适合Classification

  <img src="D:\Study\Note\机器学习\photos\image-20240915024220210.png" alt="image-20240915024220210" style="zoom:50%;" />

  - decision boundary
  - 增加了一个偏移的数据就错啦

- 引入 Logistic Regression

  - sigmoid function 

    <img src="D:\Study\Note\机器学习\photos\image-20240915024641632.png" alt="image-20240915024641632" style="zoom:33%;" />

    - 用于logistic regression

      <img src="D:\Study\Note\机器学习\photos\image-20240915024856658.png" alt="image-20240915024856658" style="zoom:33%;" />

    - <img src="D:\Study\Note\机器学习\photos\image-20240915124830972.png" alt="image-20240915124830972" style="zoom:33%;" />

    

### Decision boundary

 <img src="D:\Study\Note\机器学习\photos\image-20240916042947537.png" alt="image-20240916042947537" style="zoom:33%;" />



### Cost、Loss Function

- 若使用原来的方差cost  function，曲线不是凸函数，梯度下降无法找到最优
- <img src="D:\Study\Note\机器学习\photos\image-20240916142347584.png" alt="image-20240916142347584" style="zoom:50%;" />
- <img src="D:\Study\Note\机器学习\photos\image-20240916142241948.png" alt="image-20240916142241948" style="zoom:33%;" />

 



### Gradient descent in logistics regression

<img src="D:\Study\Note\机器学习\photos\image-20240916144730099.png" alt="image-20240916144730099" style="zoom:50%;" />

<img src="D:\Study\Note\机器学习\photos\image-20240916145029714.png" alt="image-20240916145029714" style="zoom:50%;" />



## The problem of Overfitting（过拟合）

- 一些词

  - underfit
  - bias
  - overfitting
  - generalization
  - variance

- 过拟合概念

  <img src="D:\Study\Note\机器学习\photos\image-20240916145902470.png" alt="image-20240916145902470" style="zoom:50%;" />

  <img src="D:\Study\Note\机器学习\photos\image-20240916153508637.png" alt="image-20240916153508637" style="zoom:50%;" />

- 解决方法

  1. 收集更多数据

     <img src="D:\Study\Note\机器学习\photos\image-20240916153641800.png" alt="image-20240916153641800" style="zoom:33%;" />

  2. 选择最合适的特征

     <img src="D:\Study\Note\机器学习\photos\image-20240916153921332.png" alt="image-20240916153921332" style="zoom:33%;" />

  3. Regularization（正则化）

     - 特点
       - 温和地减少部分特性的影响（而不是如上完全删除）
       - 保留了所有功能，只是防止某些功能有过大的影响



#### Cost Function with Regularization

<img src="D:\Study\Note\机器学习\photos\image-20240916162406930.png" alt="image-20240916162406930" style="zoom:50%;" />

- $\lambda$ 太小：过拟合
- $\lambda$ 太大：变直线
- 加入gradient descent
  - <img src="D:\Study\Note\机器学习\photos\image-20240916162756786.png" alt="image-20240916162756786" style="zoom:50%;" />
