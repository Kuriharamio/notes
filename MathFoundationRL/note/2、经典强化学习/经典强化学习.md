# 经典强化学习

## 1、Dynamic Programming

> DP （动态规划）指的是一种算法，可以基于一个 MDP 环境的完备模型，计算最优策略
>
> :star: 需要准确模型（转移概率 $p$ ）、大量计算，只能用于小规模问题，但却是理论基石

### :one:  动态规划算法

动态规划是一种通过**拆解问题、存储中间结果**来高效解决**重叠子问题**和**最优子结构**问题的方法

- **核心思想**

  避免重复计算，把复杂问题分解为小的子问题，记录子问题的解（用表格或数组存储），再用子问题的解推导出原问题的解

- **适用场景**

  问题可拆分为重叠子问题（子问题重复出现），且子问题的最优解能推出原问题的最优解（最优子结构）

#### 关键步骤

1. **定义状态**：确定`dp[i]`（或`dp[i][j]`）代表什么（例如：`dp[i]`表示 “前 i 个元素的最优解”）
2. **推导状态转移方程**：用`dp[i-1]`等前置状态表示`dp[i]`（核心！需要分析问题规律）
3. **初始化**：确定基础情况（如`dp[0]`、`dp[1]`的值）
4. **计算顺序**：按依赖关系依次计算（通常从左到右、从小到大）

#### 经典问题：迷宫寻路

> 给定一个包含非负整数的 $m\times n$ 网格，请找出一条从左上角到右下角的路径，使得路径上的数字总和为最小，每次只能向下或者向右移动一步
>
> ```bash
> 举例：
> 输入:
> arr = [
>   [1,3,1],
>   [1,5,1],
>   [4,2,1]
> ]
> 输出: 7
> 解释: 因为路径 1→3→1→1→1 的总和最小
> ```

1. **定义状态**

   - `dp[i][j]` 为走到该位置的时候的数字总和

2. **推导状态转移方程**

   - `dp[i][j] = dp[i-1][j]+arr[i][j]` 或 `dp[i][j] = dp[i][j-1]+arr[i][j]`
   - 为使数字总和最小，改写为 `dp[i][j] = min(dp[i-1][j], dp[i][j-1])+arr[i][j]`

3. **初始化**

   - `dp[0][0] = arr[0][0]`
   - `dp[0][j] = dp[0][j-1] + arr[0][j]`
   - `dp[i][0] = dp[i-1][0] + arr[i][0]`

4. **计算**

   ```cpp
   int minPathSum(vector<vector<int>>& grid) {
       int m = grid.size();
       int n = grid[0].size();
       vector<vector<int>> dp(m, vector<int>(n, 0));
   
       dp[0][0] = grid[0][0];
       for(int i=1; i<m; i++){
           dp[i][0] = dp[i-1][0] + grid[i][0];
       }
       for(int j=1; j<n; j++){
           dp[0][j] = dp[0][j-1] + grid[0][j];
       }
   
       for(int i=1; i<m; i++){
           for(int j=1; j<n; j++){
               dp[i][j] = min(dp[i-1][j], dp[i][j-1]) + grid[i][j];
           }
       }
   
       return dp[m-1][n-1];
   }
   ```

### :two: MDP 环境下的 DP

==基本概念==

状态 $S$、动作 $A$、转移概率 $P$、奖励 $R$、策略 $\pi$、折扣 $\gamma$

==定义状态==

- **状态价值**

  - Bellman Equation
    $$
    \begin{align}
    v_{\pi}(s_t)
    &=\mathbb E\big[ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3}+\cdots\ \big|\ S_t=s_t\big]\\
    &=\mathbb E\big[ R_{t+1} + \gamma v_\pi(s_{t+1})\ \big|\ S_t=s_t\big]\\
    &=\sum_a\pi(a|s_t)\sum_{s_{t+1},r} p(s_{t+1},r|s_t,a)\big[r + \gamma v_\pi(s_{t+1}) \big]
    \end{align}
    $$

    - 即时奖励 + 未来奖励

  - Bellman Optimal Equation

  $$
  \begin{align}
  v^*(s_t) &= \max_a\mathbb{E} \big[ R_{t+1} + \gamma v^*(s_{t+1})\ \big|\ S_t=s_t, A_t=a \big]\\
  &=\max_a\sum_{s_{t+1},r} p(s_{t+1},r|s_t,a)\big[r + \gamma v(s_{t+1}) \big]
  \end{align}
  $$

  > [!tip]
  >
  > 理解为：
  >
  > - 在状态 $s$ 下，当前策略可以采取动作空间 $A$ 中的任一动作 $a$
  > - 做出不同的动作 $a$  后，将转移到下一状态 $s_{t+1}$，并会得到即时奖励 $R_{t+1}$
  > - 由于环境多变，在同样的状态下，做出相同的动作，导致的结果可能不同，所以取期望
  > - 最后再取最大值，表示**当前策略**在**当前状态**下能达到做各种动作所能得到的**最大价值**

- **动作价值**

  - Bellman Equation
    $$
    \begin{align}
    q_\pi(s_t,a_t)&=\mathbb E\big[ R_{t+1} + \gamma v_\pi(s_{t+1})\ \big|\ S_t=s_t,A_t=a_t \big]\\
    &=\sum_{s_{t+1},r} p(s_{t+1},r|s_t,a_t)\big[r + \gamma \sum_{a_{t+1}}\pi(a_{t+1}|s_{t+1})q_\pi(s_{t+1},a_{t+1}) \big]
    \end{align}
    $$
    
  - Bellman Optimal Equation
    $$
    \begin{align}
    q^*(s_t,a_t)&=\mathbb{E} \big[ R_{t+1}+\gamma \max_{a_{t+1}} q^*(s_{t+1},a_{t+1})\ \big|\ S_t=s_t, A_t=a_t \big]\\
    &=\sum_{s_{t+1},r} p(s_{t+1},r|s_t,a_t)\big[r+\gamma \max_{a_{t+1}} q^*(s_{t+1},a_{t+1}) \big]
    \end{align}
    $$
  
  > [!tip]
  >
  > 属于是对多个动作取期望的状态价值退化为单个动作的价值
  > $$
  > v_\pi(s_t)=\sum_a\pi(a|s_t)q_\pi(s_t,a)
  > $$
  >
  > $$
  > q_\pi(s_t,a_t)=\sum_{s_{t+1},r} p(s_{t+1},r|s_t,a_t)\big[r + \gamma v_\pi(s_{t+1})\big]
  > $$
  

==推导状态转移方程==

- 根据收缩映射定理（Contraction Mapping Theorem）

  - Bellman Equation 是一个收缩映射，所以可以通过迭代方式求得不动点 $v^*$
  - 不动点满足 $f(x^*)=x^*$，故解出来的不动点就是**最优价值**，导出的策略就是**最优策略**

- 有时候旧值用不上，可以直接覆盖，就地更新，节省空间

- 下面不再使用 $s_t、a_t$，而是变为**矩阵** $s、a$，下一时刻的用 $s'、a'$ 表示

- ==**策略迭代**（基于 Bellman Equation）==

    - 基于**状态价值**
      $$
      v_{\pi}(s)=\sum_a\pi(a|s)\sum_{s',r} p(s',r|s,a)\big[r + \gamma v_\pi(s')\big]\tag{1}
      $$

      $$
      \pi^*(s)=\arg\max_a \sum_{s',r} p(s',r|s,a)\big[r + \gamma v_\pi(s')\big] \tag{2}
      $$

      $$
      (1)_{Loop\ until\  convergence}\rarr(2)\rarr(1)_{Loop\ until\  convergence}\rarr(2)\rarr\cdots
      $$

    <img src="./经典强化学习.assets/073cf23ee13e750660da4e089b559c52.png" alt="在这里插入图片描述" style="zoom:80%;" />

    - 基于**动作价值**
      $$
      q_\pi(s,a)=\sum_{s',r} p(s',r|s,a)\big[r + \gamma \sum_{a'}\pi(a'|s)q_\pi(s',a')\big]\tag{1}
      $$

      $$
      \pi^*(s)=\arg\max_a\ q_\pi(s,a) \tag{2}
      $$

      $$
      (1)_{Loop\ until\  convergence}\rarr(2)\rarr(1)_{Loop\ until\  convergence}\rarr(2)\rarr\cdots
      $$

      <img src="./经典强化学习.assets/317d6f1ee81d0b7eed401c8afbf5939b.png" alt="在这里插入图片描述" style="zoom: 80%;" />

- ==**值迭代**（基于 Bellman Optimal Equation）==

    - 基于**最优状态价值**

        $$
        v^*(s) = \max_a \sum_{s',r} p(s',r|s,a) \big[r + γ v^*(s')\big]\tag{1}
        $$

        $$
        \pi^*(s)=\arg\max_a \sum_{s',r} p(s',r|s,a)\big[r + \gamma v^*(s')\big]\tag{2}
        $$

        $$
        (1)_{Loop\ until\  convergence}\rarr(2)
        $$

        <img src="./经典强化学习.assets/2af6490afbde3fca2dc086047f5330f3.png" alt="在这里插入图片描述" style="zoom:100%;" />
        
    - 基于**最优动作价值**
        $$
        q^*(s,a)=\sum_{s',r}p(s',r|s,a)\big[r+\gamma \max_{a'}q^*(s',a') \big]\tag{1}
        $$
    
        $$
        \pi^*(s)=\arg\max_a q^*(s,a)\tag{2}
        $$
    
        $$
        (1)_{Loop\ until\  convergence}\rarr(2)
        $$

|          | Policy Iteration                   | Value Iteration                          |
| -------- | ---------------------------------- | ---------------------------------------- |
| 核心对象 | 交替优化策略和价值函数             | 直接优化价值函数，最后推导策略           |
| 迭代步骤 | 策略评估（多次价值更新）+ 策略改进 | 单次价值更新（直接用 max）+ 收敛后提策略 |
| 计算成本 | 策略评估可能耗时（需解方程组）     | 单轮迭代成本低（遍历一次状态）           |
| 收敛速度 | 策略改进步长较大，迭代次数少       | 价值更新步长较小，迭代次数可能更多       |

> [!note]
>
> **Bootstrapping**
>
> 利用后续状态的价值估计（而非真实、完整的奖励序列）来计算当前状态的价值—— ==**根据其他估计来更新估计**==
>
> - 我们通常无法获取从某个状态出发的所有未来奖励（真实环境中是无限的）
> - 所以只能用已经学到的、“不完整” 的价值函数作为临时依据，逐步迭代优化 
> - 这就像 “用自己还没完全做好的梯子，继续往上爬”
>
> **Truncated Policy Iteration**
>
> 策略迭代的策略评估步骤可以通过多种方式==截断==，仍能保证收敛性
>
> - 在策略评估阶段只执行 **$j_{truncate}$** 次迭代  
> - $j_{truncate}\rarr\infty$  → 策略迭代
> - **$j_{truncate} = 1$**  → 值迭代 （类似）
>
> **GPI**
>
> 广义策略迭代，不限定具体算法，仅强调「策略 $↔$ 值」交替更新思想
>
> - 几乎所有强化学习方法都可以很好地描述为 GPI 
> - 也就是说，所有方法都具有可识别的策略和价值函数
> - 并且策略始终相对于价值函数进行改进，而价值函数始终被驱动向当前策略的价值函数
> - 价值函数仅在与当前策略一致时才会稳定，而策略仅在相对于当前价值函数贪心时才会稳定
> - 因此，价值函数和策略必须是最优的

###  :three: References

[1] [RL(Chapter 4): Dynamic Programming (DP) (动态规划)](https://blog.csdn.net/weixin_42437114/article/details/109463115)

[2] [告别动态规划，连刷 40 道题，我总结了这些套路，看不懂你打我（万字长文）](https://zhuanlan.zhihu.com/p/91582909)



## 2、Monte Carlo Methods

> MC（蒙特卡罗方法）的核心是通过**大量随机抽样**来模拟事件结果，从而计算出难以用公式直接求解的数值答案
>
> :star: 不需要模型，但需完整轨迹，并且方差高

### :one: 蒙特卡罗方法

#### 关键步骤

1. **建立数学模型**

   将需要求解的问题（如积分、概率、最优解）转化为一个可以用随机变量描述的数学模型

2. **生成随机样本**

   通过计算机生成大量符合模型要求的随机数，模拟事件的发生

3. **统计与计算**

   统计满足目标条件的样本数量，再结合模型计算最终结果

#### 示例：计算圆周率 π

1. **构建模型**

   在边长为 $2$ 的正方形（ 坐标范围 $ [-1,1]×[-1,1]$ ）内，画一个半径为  $1$  的内切圆

   正方形面积为  $4$，圆的面积为 $ π×1²=π$

2. **生成样本**

   随机生成  $N$  个点 $（x, y）$，每个坐标都在  $[-1,1]$  之间

3. **统计计算**

   判断每个点是否落在圆内（ 满足 $x² + y² ≤ 1$ ），统计圆内点数  $M$

4. **推导结果**

   $\dfrac{圆面积}{正方形面积} ≈ \dfrac MN → \dfrac π4 ≈ \dfrac MN → π ≈ 4×(\dfrac MN)$。

### :two: MC 方法下的 RL

> 蒙特卡罗方法是基于平均样本回报来解决强化学习问题的方法
>
> 只需要 `experience` 来自与环境交互的状态、动作和奖励的**完整**样本序列，不需要对环境有完整的了解 
>
> 没有模型 → 用数据（交互产生的轨迹）代替模型，用**蒙特卡洛估计**来近似期望回报

#### 均值估计问题

|    方法    |            公式             |            依赖            |
| :--------: | :-------------------------: | :------------------------: |
| **模型式** |   $E[X] = \sum p(x)·x\\$    |    需要转移模型概率分布    |
| **无模型** | $E[X] ≈ \dfrac1n\sum x_i\\$ | 只需样本，大数定律保证收敛 |

> 状态价值、动作价值本质上都是**期望**，因此可以照搬均值估计思想
> $$
> v_\pi(s)=\mathbb E\big[\sum_{k=0}^\infty \gamma^kR_{t+k+1}\ \big|\ S_t=s\big]
> $$
> 因为模型（转移概率）不可用，所以不知道下一个状态会到哪里，因此就不能通过状态价值来评判策略的好坏，故只能通过**动作价值**来评判当前策略应该选用哪个动作，即选择 $q*(s,a)=v^*(s)$

#### 三种“visit”策略

|             策略             |                             描述                             | 样本利用率 |
| :--------------------------: | :----------------------------------------------------------: | :--------: |
| **初始访问** (initial-visit) |                 仅把整条轨迹用于起点 $(s,a)$                 |     低     |
|  **首次访问** (first-visit)  | 对同一  $(s,a) $ 只算第一次出现的回报<br />将 $v_\pi(s)$  估计为首次访问 $s$ 后的平均回报 |     中     |
|  **每次访问** (every-visit)  | 同一 $ (s,a) $ 出现多次，每次都用<br />对所有访问 $s$ 后的回报进行平均 |     高     |

<img src="./经典强化学习.assets/9d6f0a51852743fd76128fe7155f9938.png" alt="在这里插入图片描述" style="zoom:80%;" />

> [!note]
>
> - 对于 first-visit 方法，每个 return 都是对于 $v_\pi(s)$ 的有限方差的独立同分布的估计
>
>   - 根据大数定理，这些估计的平均数会收敛于期望价值，且均是无偏估计，标准差 $\dfrac1{\sqrt n}$ 随着次数增加而减小
>
> - 根据广义策略迭代（GPI）思想
>
>   - 同时维护一个近似策略和一个近似价值函数
>
>   - 近似价值函数逐步靠近当前近似策略的真实价值
>
>   - 近似策略根据近似价值不断提升优化—— ==通过贪心策略提升，即选价值最高的动作==
>     $$
>     \begin{align}
>     q_{\pi k}(s,\pi_{k+1})&=q_{\pi k}\big[s, \arg\max_a q_{\pi k}(s,a)\big]\\
>     &=\max_a q_{\pi k}(s,a)\\
>     &\geq q_{\pi k}(s, \pi_k(s))\\
>     &\geq v_{\pi k}(s)
>     \end{align}
>     $$
>     
>
>   - 两者相互作用，逐渐趋向最优
>
> - Bootstrap
>
>   - MC 方法并没有 bootstrap，一个状态的估计不建立在其他状态的估计之上，而是根据样本直接得到

#### Exploring Starts

- 每对 $(s,a)$ 都要作为起点足够多次，否则有些 $(s,a)$ 因永远都没有访问到而不会改进其估计
- 实现方法是每个 $(s,a)$ 均有非 $0$ 概率被选为起始点

<img src="./经典强化学习.assets/2a5d4688396c645f8a15420fb890ab58.png" alt="在这里插入图片描述" style="zoom:90%;" />

####  ϵ - greedy 策略

对于所有非贪心动作，选择它的概率为 $\dfrac{\epsilon}{|\mathcal A(s)|}$ ，选择贪心动作的概率为 $1-(|\mathcal A(s)|-1)\dfrac{\epsilon}{|\mathcal A(s)|}=1-\epsilon+\dfrac{\epsilon}{|\mathcal A(s)|}$

<img src="./经典强化学习.assets/083c9724b928d1ad9a45b1249edc6dd8.png" alt="在这里插入图片描述" style="zoom:100%;" />

> 因为 GPI 不要求执行的策略一定要遵循贪心策略，只需要执行的策略向贪心策略靠拢即可提升策略
> $$
> \begin{align}
> q_\pi(s,\pi'(s))&=\sum_a\pi'(a|s)q_\pi(s,a)\\
> &=\dfrac{\epsilon}{|\mathcal A(s)|}\sum_aq_\pi(s,a)+(1-\epsilon)\max_aq_\pi(s,a)\\
> &\geq\dfrac{\epsilon}{|\mathcal A(s)|}\sum_aq_\pi(s,a)+(1-\epsilon)\sum_a\dfrac{\pi(a|s)-\dfrac{\epsilon}{|\mathcal A(s)|}}{1-\epsilon}q_\pi(s,a)\\
> &=\dfrac{\epsilon}{|\mathcal A(s)|}\sum_aq_\pi(s,a)-\dfrac{\epsilon}{|\mathcal A(s)|}\sum_aq_\pi(s,a)+\sum_a\pi(a|s)q_\pi(s,a)\\
> &=v_\pi(s)
> \end{align}
> $$
>
> - 第三步使用的方法是：最大值 $\geq$ 加权平均值，系数只要保证非负且和为 $1$ 即可 
>
> 可以证明等号只能在 $\pi$ 和 $\pi'$ 均为最优策略时才能取到
>
> ==通过 $\epsilon$-greedy 策略，可以移除 Exploring Starts 的要求==

##### Exploration 与 Exploitation 权衡

|  ε 值   |        探索        |        利用（最优性）        |
| :-----: | :----------------: | :--------------------------: |
|  ε = 1  |  完全随机，强探索  |           性能最差           |
| ε = 0.1 | 轻微探索，近似最优 |         性能接近最优         |
|  ε = 0  | 无探索，易陷入局部 | 理论上最优，但可能未充分探索 |

- 经验：先用大 $ε$ 探索，再指数/线性递减 $→$ **ε-greedy 衰减策略**

#### On-Policy VS Off-Policy

> [!important]

矛盾

- 一方面需要通过最优的行为来学习动作价值
- 另一方面又需要不那么确定性、不表现得那么好，来探索所有的动作（以便找到最优动作）

解决方法

- `On-Policy`：生成数据的策略和正在改进的策略==是同一个策略==
  - 上面的方法学到的仍是 $\epsilon$-greedy 的策略，只是部署的时候一般只选择最优的（贪心）
- `Off-Policy`：生成数据的策略和正在改进的策略==不是同一个策略==
  - 拥有更大的方差，收敛得更慢，但是更加强大、有泛化性
  - 能够额外地从外部数据（传统规划数据、人类数据等）中进行学习

#### 重要性采样

> 用来估计随机变量在一个分布（目标策略 $\pi$）上的期望值，但是采样的样本来自另一个分布（行为策略 $\mathcal b$ ）==Off-Policy==

- 重要性采样率：在目标策略和行为策略下，得到某事件轨迹的概率的比值
  $$
  \rho_{t:T-1}=\dfrac{\Pi_{k=t}^{T=1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)}{\Pi_{k=t}^{T=1}b(A_k|S_k)p(S_{k+1}|S_k,A_k)}=\dfrac{\Pi_{k=t}^{T=1}\pi(A_k|S_k)}{\Pi_{k=t}^{T=1}b(A_k|S_k)}
  $$

- 根据行为策略 $b$ 得到的动作价值估计是正确的（第一步），但状态价值估计是错误的
  $$
  \mathbb E[G_t|S_t=s]=v_b(s)\neq v_\pi(s)
  $$

  - 使用**原始重要性采样**：根据采样率，将结果转换为正确的期望值
    $$
    v_\pi(s)=\mathbb E[\rho_{t:T-1}G_t|S_t=s]=\dfrac{\sum_{t\in\mathcal T(s)}\rho_{t:T-1}G_t}{|\mathcal T(s)|}
    $$

    - 对于 first-visit 方法，属于无偏估计；对于 every-visit 方法，属于有偏估计，但偏差会逐渐地趋于 $0$
    - 由于重要性采样率的方差是无界的，故 $v_\pi(s)$ 的估计的方差也是无界的

  - 使用**加权重要性采样**：
    $$
    v_\pi(s)=\dfrac{\sum_{t\in\mathcal T(s)}\rho_{t:T-1}G_t}{\sum_{t\in\mathcal T(s)}\rho_{t:T-1}}
    $$

    - 当分母为 $0$ 时，$v_\pi(s)$ 也为 $0$
    - 对于 first-visit 方法，$|\mathcal T(s)|=1$，分子分母的采样率被约掉，因此返回值直接就是 $v_b(s)$
      - 这是有偏估计，但偏差会渐进地趋于 $0$ 
      - 由于单个回报的最大权重是 $1$，因此方差也是有界且趋于 $0$ 的
    - 由于加权重要性采样方法的方差更小，故一般偏向于使用它

- 由于考虑多步情况，后续的动作价值估计也是需要修正的
  $$
  Q(s,a)=\dfrac{\sum_{t\in\mathcal T(s,a)}\rho_{t+1:T-1}G_t}{\sum_{t\in\mathcal T(s,a)}\rho_{t+1:T-1}}
  $$

<img src="./经典强化学习.assets/34054fe89da2ea8e832951f404c9e432.png" alt="在这里插入图片描述" style="zoom:100%;" />

> 以上算法还实现了增量式更新，每获得一个新的 $episode$ 都能进行更新
> $$
> v_{n+1}=v_n+\dfrac{w_n}{c_n}(G_n-v_n)\\
> c_{n+1}=c_n+w_{n+1}
> $$
>
> - $w$ 是随机的权值，可以取 $w_i=\rho_{t_i:T(t_i)-1}$
> - $c$ 是累积权值，$c_0=0$
>
> 该方法参考了**随机估计算法**
>
> - Robbins-Monro （RM）算法
>   $$
>   w_{k+1} = w_k − a_k g̃\ (w_k, η_k)
>   $$
>
>   - $a_k$ 称为学习率（步长）序列  
>
>   - 不要求知道 $g$ 的函数形式 $→ $ 黑箱优化
>
>   - 需满足 RM 收敛定理
>
>     |                     条件                     |       解释       |
>     | :------------------------------------------: | :--------------: |
>     |           $0 < c_1 ≤ g'(w) ≤ c_2$            | g 单调、斜率有界 |
>     |    $\sum a_k = ∞ \\$ 且 $\sum a_k² < ∞\\$    |  步长不过快衰减  |
>     | $ E\  [η_k\ |\ H_k\ ]=0, E\ [\ η_k^2\ ] < ∞$ | 噪声零均值且有界 |
>
>     其中：$H_k=\{\omega_k,\ \omega_{k-1},\ \ldots\ \}$
>
>     典型步长：$a_k = \dfrac1k$ 满足条件 $2$
>
> - 随机梯度下降是该算法的特例，满足该框架

- 感知折扣（Discounting-Aware）的重要性采样

  - 原始重要性采样
    $$
    v_\pi(s)=\dfrac{\sum_{t\in\mathcal T(s)}\big((1-\gamma)\sum_{h=t+1}^{T(t)-1}\gamma^{h-t-1}\rho_{t:h-1}\bar G_{t:h}+\gamma^{T(t)-t-1}\rho_{t:T(t)-1}\bar G_{t:T(t)} \big)}{|\mathcal T(s)|}
    $$

  - 加权重要性采样
    $$
    v_\pi(s)=\dfrac{\sum_{t\in\mathcal T(s)}\big((1-\gamma)\sum_{h=t+1}^{T(t)-1}\gamma^{h-t-1}\rho_{t:h-1}\bar G_{t:h}+\gamma^{T(t)-t-1}\rho_{t:T(t)-1}\bar G_{t:T(t)} \big)}{\sum_{t\in\mathcal T(s)}\big((1-\gamma)\sum_{h=t+1}^{T(t)-1}\gamma^{h-t-1}\rho_{t:h-1}+\gamma^{T(t)-t-1}\rho_{t:T(t)-1} \big)}
    $$

  通过对折扣 $\gamma$ 进行特殊处理，改善远端奖励（数值小，重要性可被降低）带来的大方差问题

  

### :three: References

[1] [RL(Chapter 5): Monte Carlo Methods (MC) (蒙特卡洛方法)](https://blog.csdn.net/weixin_42437114/article/details/109503190)



## 3、Temporal Difference

> 时序差分（TD）算法是 MC 和 DP 的思想的结合
>
> :star: 无需环境模型，从原始经验中学习；且 bootstrap，用“部分回报 + 旧估计”在线更新值函数，无需等待最终结果

### :one: TD(0) 状态值预测

对比 **MC 方法**的更新方法
$$
v(s)\larr v(s)+\alpha\big[G_t-v(s)\big]
$$

- 利用完整真实回报来更新

**TD(0) 方法**
$$
v(s)\larr v(s)+\alpha\big[r_{t+1}+\gamma v(s_{t+1})-v(s)\big]
$$

- bootstrap：利用一步奖励，结合下个状态的估计价值，来更新当前状态估计价值

- 采样更新

  - 在 DP 中，更新方法称为**期望更新**，因为其基于环境的真实状态转移分布 $p(s',r|s,a)$ 计算期望
  - 在TD中，这种更新称为采样更新，其只基于当前采样状态-动作对（单次经验采样，无需真实分布）

- 注意到TD(0)中更新值是一系列误差，这个误差称为TD误差（TD-error）
  $$
  \delta_t=r_{t+1}+\gamma v(s_{t+1})-v(s_t)
  $$

  - MC 方法中，更新误差可看作是一系列 TD-error 的和
    $$
    \begin{align} 
    G_t - v(s_t) &= (r_{t+1} + \gamma G_{t+1}) - v(s_t)
    \\ &= \underbrace{r_{t+1} + \gamma v(s_{t+1}) - v(s_t)}_{\delta_t} + \gamma \underbrace{(G_{t+1} - v(s_{t+1}))}_{\text{t+1的MC-error}} 
    \\ &= \delta_t + \gamma (G_{t+1} - v(s_{t+1})) 
    \\ &= \delta_t + \gamma \delta_{t+1} + \gamma^2 (G_{t+2} - v(s_{t+2})) 
    \\ &\quad\vdots 
    \\ &= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \dots + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t} (G_T - v(s_T))
    \\ &= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \dots + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t} (0 - 0)
    \\&=\sum_{k=t}^{T-1}\gamma^{k-t}\delta_k
    \end{align}
    $$

    - 该等式的成立条件是**回合中  $v$ 不更新**（即离线批量更新场景）
    - 若 TD 在线更新（每步修改  $v$ ），则严格等式不成立，但直观上仍体现了 MC 误差与 TD 误差的累积关系

#### 优势

- 无模型依赖
  - 相比 DP，无需环境的状态转移概率和奖励函数，直接从经验学习
- 在线增量更新
  - 相比 MC，无需等待回合结束，适用于长回合或持续任务（如实时控制）
- 收敛性保证
  - 对固定策略 $\pi$，若步长 $\alpha$ 足够小（或满足随机近似条件 $\sum \alpha_t = \infty, \sum \alpha_t^2 < \infty$），TD (0) 以概率 1 收敛到真实价值函数 $v_\pi$
- 收敛速度
  - 实践中，TD 方法在随机任务上收敛速度通常快于常数 $\alpha$  MC（使用常数 $\alpha$ 作为学习步长的增量式 MC）

#### 最优性（批量更新视角）

- 批量更新
  - 将已收集的所有经验作为 “一批数据”
  - 重复迭代更新 $v$ 直到收敛（ $\alpha$ 足够小），此时 TD (0) 和 MC 会收敛到不同的稳定点

- 批量更新下的最优性差异

| 方法        | 收敛目标                                            | 最优性特点                                                   |
| ----------- | --------------------------------------------------- | ------------------------------------------------------------ |
| 批量 MC     | 各状态实际回报的样本平均值                          | 最小化 “训练集上的均方误差”，但对未来数据的泛化性差          |
| 批量 TD (0) | 确定性等价估计（Deterministic Equivalent Estimate） | 基于最大似然模型（从经验估计状态转移和奖励）计算的价值函数，对未来数据泛化性更好 |

### :two: Sarsa

- **更新公式**  
  $$
  q(s_t,a_t) \leftarrow q(s_t,a_t) + \alpha [r_{t+1} + \gamma q(s_{t+1},a_{t+1}) - q(s_t,a_t)]
  $$

- **特点**  

  - 基于五元组 $(s_t,a_t,r_{t+1},s_{t+1},a_{t+1})$
  - **on-policy**：行为策略 $b$ 与目标策略 $π$ 相同  
  - 与策略改进结合：每步后把 $π$ 更新为 **ε-greedy(q)**


<img src="./经典强化学习.assets/sarsa.png" alt="sarsa" style="zoom: 50%;" />

### :three: Expected Sarsa

- **更新公式**
  $$
  q(s_t,a_t)\larr q(s_t,a_t)+\alpha\big[r_{t+1}+\gamma\sumπ(a|s_{t+1})q(s_{t+1},a)−q(s_t,a_t)\big]
  $$

- **特点**

  - 替代 Sarsa 中 随机选择动作 的不确定性，用目标策略下的期望动作价值更新
    - 消除随机选择的噪声，收敛更稳定
    - 在计算上比 Sarsa 更复杂
  - 可灵活切换为 on-policy 或 off-policy
    - 若行为策略 = 目标策略，则为 on-policy 
    - 若目标策略为贪婪，行为策略为探索策略，则为 off-policy

### :four: Q-learning

- **更新公式**  
  $$
  q(s_t,a_t) \leftarrow q(s_t,a_t) + \alpha [r_{t+1} + \gamma \max_a q(s_{t+1},a) - q(s_t,a_t)]
  $$

- **特点**

  - **off-policy**：行为策略 $b$ 与目标策略 $π$ **可**不同
    - **on-policy**：行为策略 = **ε-greedy**  
    - **off-policy**：先批量收集数据，再离线更新 $q$  
  - 直接学习最优动作值函数，而最优动作值函数与当前选择策略无关
  -  Maximization Bias（最大值偏差）
    - 这里的 $\max_a q(s_{t+1}, a)\\$ 是用同一组Q值既选动作又估价值
    - 当Q本身有误差时，选出来的最大值会包含这种误差，进而把偏差引入到后续的更新中，形成**系统性的高估**

<img src="./经典强化学习.assets/q-learning.png" alt="q-learning" style="zoom:50%;" />

### :five: Double Learning

- 更新公式
  $$
  q_1(s_t,a_t)←q_1(s_t,a_t)+\alpha\big[r_{t+1}+\gamma q_2(s_{t+1},\arg\max_aq_1(s_{t+1},a))−q_1(s_t,a_t)]
  $$

- **特点**

  - 解耦“选动作”和“估价值”的过程，通过维护两个独立的动作价值函数来避免偏差：
    - 用 $q_1$ 选最优动作：$a^* = \arg\max_a q_1(s,a)\\$
    - 用 $q_2$ 估计该动作的价值：$q_2(s, a^*)$
    - 两个函数交替更新，避免“用同一组Q既选又估”的问题
  - 此外还有 Sarsa 和 Expected Sarsa 的 Double Learning 版本

<img src="./经典强化学习.assets/double-q-learning.png" alt="double" style="zoom:45%;" />

### :six: n-step TD 预测

更新扩展到n步
$$
v(s)\larr v(s)+\alpha\big[r_{t+1}+\gamma r_{t+2}+\cdots+\gamma^{n} v(s_{t+n})-v(s)\big]
$$
<img src="./经典强化学习.assets/7-1-1.png" alt="7-1-1" style="zoom:50%;" />

### :seven: n-step Sarsa

- **n 步回报**  
  $$
  G_t^{(n)} = \sum_{k=1}^{n} γ^{k-1}r_{t+k} + γ^n q_t(s_{t+n},a_{t+n})
  $$

- **极端情况**  

  - $n=1$ $→$ 普通 **Sarsa**  
  - $n→∞ →$ **MC**（无 bootstrap，高方差）

- **偏差-方差权衡**：大 $n$ 降低偏差、提高方差

- 修改为期望值，则变为 n-step Expected Sarsa

<img src="./经典强化学习.assets/7-2-1.png" alt="7-2-1" style="zoom:50%;" />

### :eight: 基于重要性采样的 n-step Oﬀ-policy Learning 

- **更新公式**
  $$
  q(s_t,a_t)\larr q(s_t,a_t)+\alpha\rho_{t+1:t+n−1}\big[G_{t:t+n}−q(s_t,a_t)\big]
  $$

<img src="./经典强化学习.assets/7-3-1.png" alt="img" style="zoom:50%;" />

### :nine: 统一算法

<img src="./经典强化学习.assets/7-6-1.png" alt="img" style="zoom:50%;" />

所有 **TD** 算法可写成  
$$
q ← q + α [ ¯q − q ]
$$
其中 $¯q$ 为不同 **TD** 目标：

|       算法       |             目标             |                           关键公式                           | $¯q$ 表达式                                               |
| :--------------: | :--------------------------: | :----------------------------------------------------------: | --------------------------------------------------------- |
|    **Sarsa**     |  估计给定策略的动作值 $q_π$  |                    $q ← q + α [r+γq'−q]$                     | $r + γ q(s',a')$                                          |
| **n-step Sarsa** |   用 $n$ 步回报估计 $q_π$    |                  $q ← q + α [G_t^{(n)}−q]$                   | $\sum_{k=1}^n γ^{k-1} r_{t+k} + γ^n q(s_{t+n},a_{t+n})\\$ |
|  **Q-learning**  |      直接估计最优 $q^*$      |                 $q ← q + α [r+γ \max q'−q]$                  | $r + γ \max_{a'} q(s',a')\\$                              |
|  Expected Sarsa  | 估计给定策略的动作值 $q_\pi$ | $q \leftarrow q + \alpha \left[ r + \gamma \sum_a \pi(a|s') q(s',a) - q \right]\\$ | $r + \gamma \sum_a \pi(a|s') q(s',a)\\$                   |
