# 强化学习学习建议

## 🧭 总体导图

> 强化学习发展脉络可以分为五个阶段（兼顾机器人控制的主线）

|     阶段     |    时间    |                           算法代表                           |               特征               |       主要任务类型       |
| :----------: | :--------: | :----------------------------------------------------------: | :------------------------------: | :----------------------: |
| 1️⃣ 经典探索期 | 1980s–2000 |            Dynamic Programming, Q-Learning, SARSA            |       基础价值迭代、表格法       | Grid World, 简单离散任务 |
| 2️⃣ 深度结合期 | 2013–2016  |                 DQN, Double DQN, Dueling DQN                 |        深度网络近似Q函数         |  离散动作控制（Atari）   |
| 3️⃣ 连续控制期 | 2015–2018  |                  DDPG, TRPO, PPO, SAC, TD3                   |   可处理机器人控制（MuJoCo等）   |    机械臂、行走机器人    |
| 4️⃣ 模型增强期 | 2018–2021  |                 MBPO, Dreamer, PETS, MPC-RL                  |     引入动力学模型、混合策略     |       复杂物理仿真       |
| 5️⃣ 前沿智能期 | 2021–至今  | World Model 2/3, Gato, RT-X, <br />RoboCat, GRPO, Diffusion Policy | 大模型、模仿学习、RLHF、生成策略 |   通用机器人、具身智能   |

------

## 📘 一、经典强化学习（无需复现，只需理解原理）

> 这些算法构成 RL 的理论基础，理解 Bellman 方程、价值函数更新等核心概念即可

|             算法             |          类型           |                        流程简述                        |            要点            |            改进点            |
| :--------------------------: | :---------------------: | :----------------------------------------------------: | :------------------------: | :--------------------------: |
| **Dynamic Programming (DP)** |        基于模型         |         1. 已知转移概率；2. 价值迭代或策略迭代         |   需要模型<br />理论基石   |      只能用于小规模问题      |
|     **Monte Carlo (MC)**     |         无模型          |                通过采样完整回合估计回报                | 不需要模型<br />需完整轨迹 |            高方差            |
| **Temporal Difference (TD)** |         无模型          |               逐步更新价值函数（TD(0)）                |      结合了MC与DP优点      |          可在线学习          |
|          **SARSA**           | On-policy <br />TD控制  |   更新<br />$Q(s,a) ← Q + α(r + γQ(s’,a’) - Q(s,a))$   |         策略内学习         | 与Q-learning对比<br />更保守 |
|        **Q-learning**        | Off-policy <br />TD控制 | 更新<br />$Q(s,a) ← Q + α(r + γmax Q(s’,a’) - Q(s,a))$ |    Off-policy<br />稳定    |        奠定现代RL基础        |

> [!tip]
>
> ✅ **建议**
>  仅需理解原理和公式推导，不用复现

------

## 🧠 二、深度强化学习（DQN 系列）

> 📍**重点：离散动作控制基础；理解网络稳定性与改进策略**

|                  算法                   | 年份 |                   核心流程                   |     改进点      |          复现建议          |
| :-------------------------------------: | :--: | :------------------------------------------: | :-------------: | :------------------------: |
|                 **DQN**                 | 2015 | 使用 CNN 逼近 Q(s,a)，经验回放 + target 网络 | 深度Q学习的诞生 | ✅ 必须复现（理解经验回放） |
|             **Double DQN**              | 2016 |          使用两个网络避免Q值过估计           |   稳定性提升    |          理解即可          |
|             **Dueling DQN**             | 2016 |         拆分 Value 与 Advantage 分支         |    加快收敛     |          理解即可          |
| **Prioritized Experience Replay (PER)** | 2016 |               按TD误差采样经验               |  提升样本效率   |          理解即可          |

> [!tip]
>
> ✅ **重点复现**：DQN
> ✅ **理解改进思路**：Double、Dueling、PER 的思想

------

## ⚙️ 三、连续动作控制（机器人核心阶段）

> 📍这一阶段是**机器人强化学习的主战场**

|                     算法                      |      类型       |                           核心思路                           |                要点                |         改进点         |             复现建议             |
| :-------------------------------------------: | :-------------: | :----------------------------------------------------------: | :--------------------------------: | :--------------------: | :------------------------------: |
| **DDPG (Deep Deterministic Policy Gradient)** |  Actor-Critic   | 将 Q-learning 扩展到连续动作<br />actor 输出动作，critic 估值 | Off-policy<br />基于确定性策略梯度 | 引入目标网络、经验回放 | ✅ 必须复现<br />（理解连续控制） |
|  **TRPO (Trust Region Policy Optimization)**  | Policy Gradient |                  限制策略变化幅度（KL约束）                  |            保证单调改进            |    高开销，复杂实现    |             理解理论             |
|    **PPO (Proximal Policy Optimization)**     | Policy Gradient |                     使用裁剪损失近似TRPO                     |            稳定、易实现            |      当前主流基线      | ✅ 必须复现<br />（经典入门基线） |
|          **TD3 (Twin Delayed DDPG)**          |  Actor-Critic   |                两个critic减少过估计、延迟更新                |                稳定                |     改善DDPG稳定性     |            ✅ 必须复现            |
|          **SAC (Soft Actor-Critic)**          |   Entropy RL    |                       最大化奖励 + 熵                        |           平衡探索/利用            |  当代默认连续控制基线  |   ✅ 必须复现<br />（推荐重点）   |

> [!tip]
>
> 🧩 **学习顺序建议**：
>
> 1. DDPG → PPO → TD3 → SAC
> 2. 在 MuJoCo/IsaacGym 上实验机械臂控制、平衡车、Walker 等任务

------

## 🔬 四、模型增强与世界模型（Model-based RL）

> 📍重点在于提高**样本效率**和**泛化性**，是机器人从仿真走向现实的关键

|                    算法                    |         类型         |             思路             |      关键点      |     改进点      |                  复现建议                  |
| :----------------------------------------: | :------------------: | :--------------------------: | :--------------: | :-------------: | :----------------------------------------: |
|                  **PETS**                  | Model-based Planning |     学习动力学模型 + MPC     |    样本效率高    |   不可端到端    |                    理解                    |
| **MBPO (Model-Based Policy Optimization)** |        Hybrid        |  模型生成短轨迹用于强化学习  | 平衡准确度与效率 |     结合SAC     |       ✅ 可复现<br />（提升样本效率）       |
|        **Dreamer / DreamerV2 / V3**        |     World Model      | 通过潜变量模型在潜空间中规划 |  学习“梦境”环境  |   从像素学习    | ✅ 推荐复现 DreamerV2<br />（具身智能重要） |
|                 **PlaNet**                 |     世界模型先驱     |       学习隐空间动力学       | 学习隐空间动力学 | 奠定Dreamer基础 |                  理解即可                  |

------

## 🤖 五、机器人与具身智能的前沿 RL

|                算法 / 系统                |              核心创新               |              要点              |       复现建议       |
| :---------------------------------------: | :---------------------------------: | :----------------------------: | :------------------: |
|         **Gato (DeepMind, 2022)**         |      多模态多任务 Transformer       | 同一模型控制机器人、语言、图像 |       理解框架       |
|         **RT-1 / RT-2 (Google)**          | 视觉-语言-动作模型，模仿学习 + RLHF |       大规模机器人数据集       |         理解         |
|       **RoboCat (DeepMind, 2023)**        |             元学习 + RL             |      通过多任务提升自适应      |         理解         |
|     **Diffusion Policy (2023–2025)**      |      通过扩散模型生成动作序列       |          平滑控制策略          | ✅ 推荐复现（新范式） |
|           **GRPO / GRT (2024)**           |       结合LLM与RL（RLHF形式）       |        强调通用策略学习        |         理解         |
| **World Model 3 / DreamerV3 (2023–2025)** |           泛化型世界模型            |        一体化模仿 + RL         |    ✅ 推荐深入学习    |

------

## 🧩 建议的学习与复现路线（Robot RL方向）

|    阶段    |             算法             |        目标        |     是否复现      |
| :--------: | :--------------------------: | :----------------: | :---------------: |
| ① 理论打底 |    Q-learning, SARSA, TD     |  理解Bellman更新   |         ❌         |
| ② 深度入门 |       DQN, Double DQN        |   理解深度Q学习    |         ✅         |
| ③ 连续控制 |     DDPG, PPO, TD3, SAC      |  核心强化学习算法  |        ✅✅✅        |
| ④ 提升效率 |       MBPO, DreamerV2        | 模型增强、世界模型 | ✅（Dreamer 推荐） |
| ⑤ 前沿探索 | Diffusion Policy, RT-2, Gato |    通用智能方向    |   ✅（选1研究）    |

