# 强化学习基础

------

[TOC]

<div style="page-break-after:always"></div>

## 第一章：基础概念

### 1. 状态与动作（State & Action）

- **状态（State）**
  - 所有状态的集合称为**状态空间**，记为：$\mathcal S = \{s_1, \dots, s_k\}$
- **动作（Action）**
  - 所有动作的集合称为**动作空间**，记为：$\mathcal A = \{a_1, \dots, a_k\}$
  - 不同状态可以有不同的动作空间（例如边界状态的动作受限）

---

### 2. 状态转移（State Transition）

- **定义**
  - 在某一状态下执行某一动作后，转移到下一状态的过程
  - 示例：$s_1 \xrightarrow{a_2} s_2$（在 $s_1$ 状态下，执行 $a_2$ 动作，到达 $s_2$ 状态）
- **状态转移表（Tabular Representation）**

| 状态\动作 | $a_1$ | $a_2$ | $a_3$ | $a_4$ | $a_5$ |
| :-------: | :---: | :--: | :--: | :--: | :--: |
|   $s_1$    | $s_1$  | $s_2$ | $s_4$ | $s_1$ | $s_1$ |
|   $s_2$    | $s_2$  | $s_3$ | $s_5$ | $s_1$ | $s_2$ |
|    $...$    |  $...$  | $...$ | $...$ | $...$ | $...$ |

- **概率表示**：
  - 一般地，状态转移可用条件概率描述：$p(s'|s, a)$ 

---

### 3. 策略（Policy）

- **直观理解**
  - 策略告诉机器人在每个状态下应采取何种动作
- **数学定义**
  - 策略 $\pi(a|s)$ ：在状态 $s$ 下选择动作 $a$ 的概率分布
  - **确定性策略**：某一动作概率为 $1$，其他为 $0$
  - **随机性策略**：多个动作有非零概率
- **策略表示表（Tabular Representation）**：

| 状态\动作 |  $a_1$  |  $a_2$  |  $a_3$  |  $a_4$  |  $a_5$  |
| :-------: | :--: | :--: | :--: | :--: | :--: |
|    $s_1$    |  $0$  | $0.5$ | $0.5$ |  $0$  |  $0$  |
|    $s_2$    |  $0$  |  $0$  |  $1$  |  $0$  |  $0$  |
|    $...$    | $...$ | $...$ | $...$ | $...$ | $...$ |

---

### 4. 奖励（Reward）

- **定义**
  - 执行动作后从环境获得的即时反馈，记为 $r(s,a)$
- **奖励表示表（Tabular Representation）**

| 状态\动作 |  $a_1$  |  $a_2$  |  $a_3$  |  $a_4$  |  $a_5$  |
| :-------: | :--: | :--: | :--: | :--: | :--: |
|    $s_1$    |  $-1$  |  $0$  |  $0$  |  $-1$  |  $0$  |
|    $s_2$    |  $-1$  |  $0$  |  $0$  |  $0$  |  $0$  |
|    $...$    | $...$ | $...$ | $...$ | $...$ | $...$ |

- **概率表示**
  - 一般地，奖励可用条件概率描述：$p(r=R|s, a)$ 

- **注意事项**
  - 奖励设计对策略学习至关重要
  - 不能仅根据即时奖励选择动作，需考虑长期累积奖励

---

### 5. 轨迹、回报与幕（Trajectory, Return & Episode）

- **轨迹（Trajectory）**
  
  - 状态-动作-奖励的序列，如：
    - $s_1 \xrightarrow{a_2,\ r=0} s_2 \xrightarrow{a_3,\ r=0} s_5 \xrightarrow{a_3,\ r=0} s_8 \xrightarrow{a_2,\ r=+1} s_9$
  
- **回报（Return）**
  
  - 轨迹中所有奖励的总和
  
  - 示例：上述轨迹的回报为：$0 + 0 + 0 + 1 = 1$
    
  - 引入**折扣回报**（Discounted Return）
    - 折扣率$\gamma \in (0,1)$，公式
      $$
      \text{折扣回报} = \sum_{t=0}^{\infty} \gamma^t r_t
      $$
  
- **回合（Episode）**
  
  - 从初始状态到终止状态（如目标状态）的一次完整交互过程
  - **回合式任务（Episodic Task）**：有明确的终止状态
  - **连续任务（Continuing Task）**：无终止状态

---

### 6. 马尔可夫决策过程（MDP）

- **定义**
  
  - 强化学习的数学框架，包含以下要素
    - 集合
      - **状态空间** $\mathcal S$
      - **动作空间 **$\mathcal A(s)$（每个状态$s$对应）
      - **奖励集合** $\mathcal R(s,a)$
    - 模型
      - **状态转移概率** $p(s'|s,a)$
      - **奖励概率** $p(r|s,a)$
    - 策略
      -  $\pi(a|s)$
  
- **马尔可夫性质（Markov Property）**
  
  - 下一状态或奖励仅依赖于当前状态和动作，与历史无关：
    $$
    p(s_{t+1}|s_t,a_t,\dots,s_0,a_0) = p(s_{t+1}|s_t,a_t)
    $$
  
- **与马尔可夫过程（MP）的区别**：
  - MDP固定策略后，退化为MP（马尔可夫链）

---

### 7. Q&A

|              问题              |                             回答                             |
| :----------------------------: | :----------------------------------------------------------: |
| 能否将所有奖励设为全正或全负？ | 奖励的绝对值不重要，相对值决定策略<br>可对奖励整体加减常数，最优策略不变 |
|    奖励是否依赖于下一状态？    | 奖励实际依赖于当前状态、动作和下一状态<br>但可简化为仅依赖当前状态与动作，因下一状态由当前状态与动作决定 |

<div style="page-break-after:always"></div>

## 第二章：状态值和贝尔曼方程

### 1. 状态价值 $v_π(s)$

- 随机轨迹：$S_t\xrightarrow{A_t}S_{t+1},R_{t+1}\xrightarrow{A_{t+1}}S_{t+2},R_{t+2}\xrightarrow{A_{t+2}}S_{t+3},R_{t+3}\ \ldots$

- 折扣回报：  
  $$
  G_t \doteq R_{t+1} + γR_{t+2} + γ²R_{t+3} + …
  $$

- **状态价值**： 
  $$
  v_π(s) \doteq E[G_t | S_t = s]
  $$
  
  - 依赖于 $s、\pi$，不依赖于 $t$
  - 当环境或策略是 deterministic，轨迹是确定的，$v_\pi(s)$ 也是确定的
  - 当环境或策略是 stochastic，轨迹不尽相同，$v_\pi(s)$ 为所有轨迹的 return 的均值

---

### 2. 贝尔曼方程（Bellman Equation）

#### 2.1 推导思路

从 $G_{t} = R_{t+1} + γG_{t+1}$ 两边取条件期望得：

$$
\begin{align}
v_\pi(s) 
&= \underbrace{\sum_{a\in\mathcal A(s)} \pi(a|s) \sum_{r\in\mathcal R(s,a)} p(r|s,a)\,r}_{\text{即时奖励期望}} + \gamma \underbrace{\sum_{s'\in\mathcal S} v_\pi(s')\sum_{a\in\mathcal A(s)} p(s'|s,a)\,\pi(a|s) }_{\text{未来奖励期望}}\\
&=\sum_{a\in\mathcal A(s)} \pi(a|s)\bigg[\sum_{r\in\mathcal R(s,a)} p(r|s,a)\,r+\gamma \sum_{s'\in\mathcal S} p(s'|s,a)\,v_\pi(s')\, \bigg]\\\\
&\text{for\ all\ }s\in\mathcal S
\end{align}
$$

---

#### 2.2 两种等价写法

|        写法         |                             公式                             |
| :-----------------: | :----------------------------------------------------------: |
|  **$(s,a)$ 联合**   | $v_\pi(s) = \sum_{a\in\mathcal A(s)} \pi(a\vert s)\sum_{s'\in\mathcal S} \sum_{r\in\mathcal R(s,a)}p(s',r\vert s,a)\big[r + \gamma v_\pi(s')\big]\\$ |
| **$r$ 仅依赖 $s'$** | $v_\pi(s) =  \sum_{a\in\mathcal A(s)} \pi(a\vert s)\sum_{s'\in\mathcal S}p(s'\vert s,a)\big[r(s') + \gamma v_\pi(s')\big]\\$ |

---

### 3. 矩阵-向量形式

- 定义：  
  $$
  v_{\pi} = r_π + γP_π v_\pi
  $$

  - $v = [v(s_1),…,v(s_n)]^T$

  - $r_π = [r_π(s_1),…,r_π(s_n)]^T$ 

  - $P_π$ 为状态转移矩阵，$[P_{\pi}]_{ij}=p_{\pi}(s_j\vert s_i)$，满足 $P_π[1,\ldots,1]^T= [1,\ldots,1]^T$

---

### 4. 求解方法

|    方法    |             公式              |          说明          |
| :--------: | :---------------------------: | :--------------------: |
| **闭式解** | $v = (I-\gamma P_π)^{-1} r_π$ | 理论分析用，需矩阵求逆 |
| **迭代解** |  $v_{k+1} = r_π + γP_π v_k$   |  实际算法用，保证收敛  |

- 当 $k\rarr\infty$ 时，$v_k\rarr v_{\pi}$

---

### 5. 动作价值 $q_π(s,a)$

#### 5.1 定义

$$
q_π(s,a) \doteq E[G_t \vert S_t = s, A_t = a]
$$

#### 5.2 与 vπ 的关系

| 方向    | 公式                      |
| :-----: | :-----------------------: |
| $v_π ← q_π$ | $v_\pi(s) = \sum_{a\in\mathcal A} \pi(a\vert s) q_\pi(s,a)\\$ |
| $q_π ← v_π$ | $q_\pi(s,a) = \sum_{r\in\mathcal R} p(r\vert s,a)r + \gamma \sum_{s'\in\mathcal S} p(s'\vert s,a)v_\pi(s')\\$ |

#### 5.3 常见误区

- **误区**：未被 $π$ 选中的动作可忽略或设 $0$。  
- **正解**：所有动作都有 $q$ 值；评估时需**全部计算**，以便后续改进策略。

---

### 6. 动作价值的贝尔曼方程

将 $v_π(s')=∑_{a'}π(a'|s')q_π(s',a')$ 代入 $q_π$ 表达式得：

$$
q_\pi(s,a) = \sum_{r\in\mathcal R} p(r\vert s,a)r + \gamma \sum_{s'\in\mathcal S} p(s'\vert s,a)\sum_{a'\in\mathcal A(s')} \pi(a'\vert s') q_\pi(s',a')
$$
矩阵形式：

$$
q_\pi = \tilde{r} + \gamma P \Pi q
$$

- 其中，$[q_\pi]_{(s,a)}=q_\pi(s,a)、[\tilde r]_{(s,a)}=\sum_{r\in\mathcal R}p(r\vert s,a)r、[P]_{(s,a),s'}=p(s'\vert s,a)、\Pi_{s’,(s’,a’)}=\pi(a’\vert s’)\\$
- **与状态价值的贝尔曼方程不同于**：$\tilde{r}$、$P$ 与 $π$ 无关；策略信息全在 $\Pi$ 中。

---

### 7. 小结

- **状态价值**是量化策略优劣的核心指标。  
- **贝尔曼方程**揭示了状态价值的自洽结构，是**策略评估**的理论基石。  
- **动作价值**为后续改进策略（第3章最优方程，第4章迭代算法）提供直接依据。  
- 贝尔曼思想不仅用于 **RL**，也普遍存在于控制论、运筹学等领域。

---

### 8. Q&A

|            问题            |                             答案                             |
| :------------------------: | :----------------------------------------------------------: |
|    状态价值与回报关系？    |            状态价值 = 从该状态出发所有回报的期望             |
|      为何关注状态价值      |                    状态价值可用于评估策略                    |
|     为何学贝尔曼方程？     |             它是计算/分析状态价值的**系统工具**              |
|  为何关注未选动作的价值？  |              它们可能是**更优动作**，需持续探索              |
| 状态价值与动作价值的关系？ | 状态价值是当前状态的动作价值的期望<br>动作价值依赖于执行该动作后下个状态的状态值 |

<div style="page-break-after:always"></div>

## 第三章：最优状态值与贝尔曼最优方程

### 1. 如何改进策略？

- **数学验证**：
  1. 先求当前策略的状态值 $v_π$。  
  2. 计算 $s_1$ 的所有动作价值 $q_π(s_1,a)$。  
  3. 选 $q$ 最大的动作 $a$，即可得到更好的策略。  
- **“选 q 最大的动作”**是策略改进的核心思想。

---

### 2. 最优策略与最优状态值

|          概念           |                     数学描述                      |
| :---------------------: | :-----------------------------------------------: |
|   **最优策略 $π^*$**    | 对所有状态 s，满足 $v_{π^*}(s) ≥ v_π(s)\quad(∀π)$ |
| **最优状态值 $v^*(s)$** |    $v_{π^*}(s)$ 的值，即所有策略中最大的状态值    |

---

### 3. 贝尔曼最优方程（BOE）

对任意状态 s：

$$
\begin{align}
v(s) 
&= \max_{\pi(s)\in\Pi(s)} \sum_{a\in\mathcal A} \pi(a\vert s)\Bigl(\sum_{r\in\mathcal R} p(r\vert s,a)r + \gamma \sum_{s'\in\mathcal S} p(s'\vert s,a)v(s')\Bigr)\\
&= \max_{\pi(s)\in\Pi(s)} \sum_{a\in\mathcal A} \pi(a\vert s) q(s,a)
\end{align}
$$
其中
$$
q(s,a) \doteq \sum_{r\in\mathcal R} p(r\vert s,a)r + \gamma \sum_{s'\in\mathcal S} p(s'\vert s,a)v(s')
$$
矩阵-向量形式
$$
v=f(v) \doteq \max_{\pi(s)\in\Pi(s)}  (r_π + γP_π v)
$$

---

### 4. 收缩映射定理（Contraction Mapping Theorem）

- 存在 $\gamma \in (0,1)$ 使得
  $$
  \norm{f(x_1)-f(x_2)}\leq\gamma\norm{x_1-x_2}
  $$

  - 则该函数为收缩映射，且存在唯一一个不动点 $x^*$，使得 $f(x^*)=x^*$，且该不动点可通过迭代方式求得：$x_{k+1}=f(x_k)$，当 $k\rarr\infty$ 时，$x_k\rarr x^*$

- **BOE** 是一个收缩映射，因此可以用迭代方式求解不动点 $v^*$

|    结论    |                    说明                    |
| :--------: | :----------------------------------------: |
| **存在性** |            BOE 总有唯一解 $v^*$            |
| **唯一性** |           最优策略值 $v^*$ 唯一            |
|  **算法**  | 迭代 $v_{k+1} = f(v_k)$ 必收敛且指数级快速 |
| **最优性** | 由 $v^*$ 导出的贪婪策略 $π^*$ 就是最优策略 |

---

### 5. 如何由 BOE 求最优策略

1. **求 $v^*$** 
   $$
   迭代：v_{k+1} = \max_{π\in\Pi} (r_π + γP_π v_k)  → 值迭代
   $$
   
2. **求 $π^*$** 
   $$
   π^* = \arg\max_{π\in\Pi} (r_π + γP_π v^*)
   $$
   等价于：对每个 $s$ 取 $a^*(s)=\arg\max_a q^*(s,a)$

---

### 6. 最优策略的性质

|    问题    |               答案               |
| :--------: | :------------------------------: |
| **存在性** |             总是存在             |
| **唯一性** |   $v^*$ 唯一，$π^*$ 可能不唯一   |
| **确定性** | 总存在一个**确定性贪婪最优策略** |
| **随机性** |    允许随机最优策略，但非必要    |

---

### 7. 影响最优策略的因素

#### 7.1 折扣率 $γ$ 的影响

| $γ$ 值  |           策略特征           |
| :-----: | :--------------------------: |
| $γ=0.9$ | **远视**，敢穿越禁区节省路径 |
| $γ=0.5$ |    **中庸**，绕远路避禁区    |
|  $γ=0$  |    **近视**，只看即时奖励    |

#### 7.2 奖励 r 的影响

- **增大禁区惩罚**（$r_{forbidden}$ 从 $−1→−10$）→ 策略完全避开禁区。  
- **仿射变换不变性**：对所有奖励同时加常数或乘正数，**最优策略不变**。

#### 7.3. 无意义绕路问题

- **折扣率本身即可抑制绕路**：
  即使每步奖励 0，折扣因子也会惩罚长轨迹，让最优策略自然选择最短路径。

---

### 8. 总结

- 本章的核心概念包括最优策略和最优状态值
- 如果一个策略的状态值大于或等于任何其他策略的状态值，则该策略是最优的
- 最优策略的状态值即为最优状态值
- BOE 是分析最优策略和最优状态值的核心工具。该方程是一个具有良好收缩性质的非线性方程。我们可以应用收缩映射定理来分析该方程。
- 结果表明，BOE 的解对应于最优状态值和最优策略

------

### 9. Q&A 

|               提问               |                   回答                   |
| :------------------------------: | :--------------------------------------: |
|       最优策略一定存在吗？       |           是的，BOE 保证存在。           |
|         最优策略唯一吗？         |          值唯一，策略可能多个。          |
|    最优策略一定是确定性的吗？    |     至少存在一个确定性贪婪最优策略。     |
|          γ 变小会怎样？          |          策略更近视，不愿冒险。          |
| 奖励整体加常数会改变最优策略吗？ |        不会，仿射变换保策略不变。        |
|          如何避免绕路？          | 折扣率已自动抑制长绕路，无需额外负奖励。 |

<div style="page-break-after:always"></div>

## 第四章：值迭代与策略迭代

### 1. 值迭代（Value Iteration）

#### 1.1 数学形式

- **矩阵-向量形式** 
  $$
  v_{k+1} = \max_{\pi\}(r_\pi + \gamma P_\pi v_k)
  $$

- **元素形式** 
  $$
  v_{k+1}(s) = \max_a \Bigl[\sum_r p(r|s,a)r + \gamma \sum_{s'}p(s'|s,a)v_k(s')\Bigr]
  $$

#### 2.2 算法流程（Algorithm 4.1）

1. 初始化 $v_0$
2. 重复直到收敛 
   • **策略更新**（隐式）：$π_{k+1}(s)=\arg\max_a q_k(s,a)$
   • **值更新**：$v_{k+1}(s)=\max_a q_k(s,a)$

> 注意：中间 vk 并非任何策略的真实状态值，仅算法变量。

---

### 3. 策略迭代（Policy Iteration）

#### 3.1 算法流程（Algorithm 4.2）

每轮迭代含两步：

1. **策略评估** 
   给定 $π_k$，通过贝尔曼方程求 $v_{π_k}$
   - 闭式：$v_{π_k}=(I-γP_{π_k})^{-1}r_{π_k}$ （理论用）  
   - 迭代：$v^{(j+1)}_{π_k}=r_{π_k}+γP_{π_k}v^{(j)}_{π_k}$ （实践用）

2. **策略改进** 
   $π_{k+1}(s)=\arg\max_a q_{π_k}(s,a)$

#### 3.2 收敛性

- **策略改进引理**：$v_{π_{k+1}} ≥ v_{π_k}$（逐元素）  
- **单调收敛定理**：策略值序列非降且有上界 $v^*$ → 必收敛到 $v^*$  
- **定理 4.1**：${v_{π_k}} → v^*，{π_k}$ $→$ 最优策略

---

### 4. 截断策略迭代（Truncated Policy Iteration）

#### 4.1 统一视角

- 在策略评估阶段只执行 **$j_{truncate}$** 次迭代  
- **$j_{truncate} = 1$**  → 值迭代  
- $j_{truncate}\rarr\infty$  → 策略迭代

#### 4.2 算法（Algorithm 4.3）

1. 初始化 $π_0$
2. 循环直到收敛 
   • **截断策略评估**：用 $v_{k−1}$ 作初值，迭代 $j_{truncate}$ 次得 $v_k$ 
   • **策略改进**：$π_{k+1}(s)=\arg\max_a q_k(s,a)$

> 实际经验：$j_{truncate}$ 取 3–10 即可显著加速收敛。

---

### 5. 三种算法对比

|     维度     |    值迭代    |   策略迭代   |      截断策略迭代      |
| :----------: | :----------: | :----------: | :--------------------: |
| 每轮策略评估 |     1 步     |     精确     | 有限 $j_{truncate}$ 步 |
|  计算量/轮   |      低      |      高      |           中           |
|   收敛轮数   |      多      |      少      |        介于之间        |
|   实现难度   |      低      |      中      |           中           |
|   适用场景   | 状态空间巨大 | 状态空间中等 |          通用          |

---

### 6. 关键概念

|          概念           |                       解释                        |
| :---------------------: | :-----------------------------------------------: |
| **广义策略迭代（GPI）** | 不限定具体算法，仅强调「策略 $↔$ 值」交替更新思想 |
|   **动态规划（DP）**    |     本章节算法统称，需已知模型 $p(r,s'|s,a)$      |
|    **模型无关扩展**     | Ch5 起将用采样替代模型，延伸出蒙特卡洛、TD 等算法 |

---

### 7. 实现小贴士

- **收敛判据**：$∥v_{k+1}-v_k∥_∞ < ε$ ( $ε≈$ 1e-6 )
- **策略评估迭代初值**：上一轮 $v_{k-1}$ 可显著减少迭代次数  
- **并行化**：值/策略迭代的「对每个 s」步骤天然可并行

---

### 8. 小结

|       算法       |                      一句话总结                       |
| :--------------: | :---------------------------------------------------: |
|    **值迭代**    | 用 Bellman optimality equation 做不动点迭代，简单高效 |
|   **策略迭代**   |   交替“评估+改进”，每轮精确评估，收敛快但每轮成本高   |
| **截断策略迭代** |          折中方案，用有限步评估换取整体效率           |

---

### 9. Q&A 

|                  问题                  |                       答案                        |
| :------------------------------------: | :-----------------------------------------------: |
|        值迭代一定能收敛到最优？        |               是，收缩映射定理保证                |
|         策略迭代为何嵌套迭代？         |         策略评估需解线性方程，可迭代求解          |
| 截断策略迭代的 $j_{truncate}$ 如何选？ |          经验值 3–10，越大越接近策略迭代          |
|        这些算法为何不算 “RL” ？        | 它们需要模型，属于动态规划；后续无模型算法才称 RL |
|          广义策略迭代指什么？          |       所有“价值 $↔$ 策略”交替更新的思想统称       |

<div style="page-break-after:always"></div>

## 第五章：蒙特卡洛方法

### 1. 核心思想一句话

**“没有模型 → 用数据（交互产生的轨迹）代替模型，用**蒙特卡洛估计**来近似期望回报。”**

---

### 2. 动机：均值估计问题

|    方法    |            公式             |            依赖            |
| :--------: | :-------------------------: | :------------------------: |
| **模型式** |   $E[X] = \sum p(x)·x\\$    |        需要概率分布        |
| **无模型** | $E[X] ≈ \dfrac1n\sum x_i\\$ | 只需样本，大数定律保证收敛 |

> 状态/动作值本质上都是**期望回报**，因此可以照搬均值估计思想。

---

### 3. MC Basic：最简单的 MC-RL 算法

#### 3.1 算法结构（Algorithm 5.1）

|   步骤   |    原策略迭代    |                     MC Basic 的替换                     |
| :------: | :--------------: | :-----------------------------------------------------: |
| 策略评估 |  用模型求 $v_π$  | 对每 $(s,a)$ 采集 $n$ 条轨迹，用平均回报估计 $q_π(s,a)$ |
| 策略改进 | $π ← \arg\max q$ |                          同上                           |

---

#### 3.2 伪代码



- **优点**：概念清晰、实现简单  
- **缺点**：每个 (s,a) 都要大量轨迹 → 样本效率极低

---

#### 3.3 要点

- **Episode 长度**必须足够 → 否则无法捕获折扣回报 
- **稀疏奖励问题**：正奖励仅出现在目标，需长轨迹；可通过设计**稠密奖励**缓解

---

### 4. MC Exploring Starts：提高样本利用率

#### 4.1 三种“visit”策略

|             策略             |               描述                | 样本利用率 |
| :--------------------------: | :-------------------------------: | :--------: |
| **初始访问** (initial-visit) |    仅把整条轨迹用于起点 (s,a)     |     低     |
|  **首次访问** (first-visit)  | 对同一 (s,a) 只算第一次出现的回报 |     中     |
|  **每次访问** (every-visit)  |   同一 (s,a) 出现多次，每次都用   |     高     |

#### 4.2 算法

- 利用**子轨迹**（从中间状态开始继续到结束）  
- **回溯计算**折扣回报：从尾部向前累加  
- 仍需 **Exploring Starts** —— 每 $(s,a)$ 都要作为起点足够多次

---

### 5. MC ε-Greedy：去掉 Exploring Starts

#### 5.1 核心技巧：软策略（soft policy）

- **ε-greedy 定义**

$$
π(a|s) =
\begin{cases}
1-\dfrac\epsilon{|A|}(|A(s)|-1) \qquad (若 a = \arg\max)\\
\dfrac{ε}{|A|}\qquad\qquad\qquad\quad\qquad(其他动作)\\
\end{cases}
$$

- **作用**  
  - $ε>0$：保证持续探索，无需人为指定起点  
  - $ε→0$：逐渐逼近贪婪最优策略

#### 5.2 算法

- 每生成一条完整轨迹后，**立即**用其更新所有出现的 $(s,a)$ 的 $q$ 值  
- 策略改进时直接更新为 **ε-greedy** 分布

#### 5.3 收敛说明

- **在 $Π_ε$ 内**的最优策略可收敛  
- $ε$ 越小，越接近全局最优；但探索减弱 $→$ 通常采用 $ε$ 衰减策略

#### 5.4. Exploration vs. Exploitation 权衡

|  ε 值   |        探索        |        利用（最优性）        |
| :-----: | :----------------: | :--------------------------: |
|  ε = 1  |  完全随机，强探索  |           性能最差           |
| ε = 0.1 | 轻微探索，近似最优 |         性能接近最优         |
|  ε = 0  | 无探索，易陷入局部 | 理论上最优，但可能未充分探索 |

> 经验：先用大 $ε$ 探索，再指数/线性递减 $→$ **ε-greedy 衰减策略**

---

### 6. 小结

|          算法           |                    一句话总结                     |
| :---------------------: | :-----------------------------------------------: |
|      **MC Basic**       |  用采样平均回报直接估计 $q$ ，概念简单但样本爆炸  |
| **MC Exploring Starts** | 用 `first/every-visit` 提高样本效率，仍需人为起点 |
|     **MC ε-Greedy**     |  引入 `ε-greedy` 软策略，无需人为起点，实用性强   |

---

### 7. Q&A

|            提问             |                     回答                     |
| :-------------------------: | :------------------------------------------: |
|     MC 方法与模型关系？     |         无需转移概率，完全用交互数据         |
|     为何先学 MC Basic？     | 揭示 “无模型 $≈$ 用采样替换模型” 的核心思想  |
|     Episode 长度要求？      |     需足够长以覆盖折扣回报；稀疏奖励尤甚     |
| Exploring Starts 能否避免？ |          用 `ε-greedy` 等软策略即可          |
|         ε 如何调？          | 先大后小，常用指数衰减 $ε_t = ε_0·\exp(-λt)$ |

<div style="page-break-after:always"></div>

## 第六章：随机估计算法

### 1. 核心目标

- 把 **非增量式** 的蒙特卡洛估计 $→$ **增量式在线更新**  
- 建立 **Robbins-Monro (RM) 算法** 与 **随机梯度下降 (SGD)** 的统一框架  
- 用 **Dvoretzky 定理** 证明一类增量迭代的收敛性

---

### 2. 动机：均值估计的增量化

|                非增量式                |                   增量式                   |
| :------------------------------------: | :----------------------------------------: |
| $x̄ = \dfrac1n \sum x_i\\$ 需存全部样本 | $w_{k+1} = w_k − α_k (w_k − x_k)$ 每步更新 |

​	等 $α_k = \dfrac1k$ 时，两者等价

------

### 3. 随机逼近（Stochastic Approximation）

#### 3.1 问题形式

求根 $g\ (w)=0$，但只能观测含噪值  $g̃\ (w, η) = g\ (w) + η$

#### 3.2 Robbins-Monro 算法（RM）

$$
w_{k+1} = w_k − a_k g̃\ (w_k, η_k)
$$

- $a_k$ 称为学习率（步长）序列  
- 不要求知道 $g$ 的函数形式 $→ $ 黑箱优化

#### 3.3 RM 收敛定理

|                     条件                     |       解释       |
| :------------------------------------------: | :--------------: |
|            $0 < c_1 ≤ g'(w) ≤ c_2$             | g 单调、斜率有界 |
|    $\sum a_k = ∞ \\$ 且 $\sum a_k² < ∞\\$    |  步长不过快衰减  |
| $ E\  [η_k\ |\ H_k\ ]=0, E\ [\ η_k^2\ ] < ∞$ | 噪声零均值且有界 |

其中：$H_k=\{\omega_k,\ \omega_{k-1},\ \ldots\ \}$

> 典型步长：$a_k = \dfrac1k$ 满足条件 $2$

---

### 4. Dvoretzky 定理：更一般的收敛工具

- 统一处理 **随机矩阵系数** 与 **多维变量**  
- 形式：

$$
Δ_{k+1} = (1-α_k)Δ_k + β_k η_k
$$

- 若满足：
  $$
  \sum_{k=1}^\infty α_k = ∞\qquad \sum_{k=1}^\infty α_k² < ∞\qquad \sum_{k=1}^\infty β_k² < ∞  \\
  E\ [\ η_k\ |\ H_k\ ]=0\qquad E\ [\ η_k^2\ |\ H_k\ ] ≤ C
  $$
  其中：$H_k=\{\Delta_k,\ \Delta_{k-1},\ \ldots,\ \eta_{k-1},\ \ldots,\ \alpha_{k-1},\ \ldots,\ \beta_{k-1},\ \ldots\ \}$

> 第 7 章将用其证明 Q-learning、TD(λ) 等算法收敛。

---

### 5. 随机梯度下降（SGD）

#### 5.1 标准形式

优化目标：
$$
\min_w J(w) = E[f(w,X)]
$$

- **SGD** 更新：

$$
w_{k+1} = w_k − α_k ∇_w f(w_k, x_k)
$$



- 只需 **单样本梯度** $→$ 在线学习

#### 5.2 SGD 是 RM 的特例

- 令 $g(w)=∇J(w)$，则 **SGD** 服从 **RM** 框架  
- 收敛条件同 **RM** + 凸性/曲率有界

---

### 6. 三大梯度下降对比

|       算法        |  每步样本   | 方差 |   适用场景   |
| :---------------: | :---------: | :--: | :----------: |
|      **BGD**      |  全部 $n$   | 最小 | 小数据、精确 |
|      **SGD**      |     $1$     | 最大 | 大数据、在线 |
| **Mini-batch GD** | $m (1<m<n)$ |  中  |   折中方案   |

---

### 7. 关键公式

|        场景         |                             公式                             |
| :-----------------: | :----------------------------------------------------------: |
|   **RM** 通用形式   |             $w_{k+1} = w_k − a_k g̃\ (w_k, η_k)$              |
| 均值估计（**SGD**） |              $w_{k+1} = w_k − α_k (w_k − x_k)$               |
| **Dvoretzky** 条件  | $\sum_{k=1}^\infty α_k = ∞ \quad\sum_{k=1}^\infty α_k² < ∞\quad\sum_{k=1}^\infty  β_k² < ∞\\$ |

---

### 8. 与后续章节的桥梁

- **TD 学习** 可写成 **RM / SGD** 形式  
- **Q-learning** 可视为 **Dvoretzky** 定理的特例  
- 所有增量式值函数更新都逃不出本章框架

---

### 9. 小结

|        工具        |             作用             |
| :----------------: | :--------------------------: |
|    **RM 算法**     |    黑盒求根的增量迭代框架    |
| **Dvoretzky 定理** |   证明多维增量迭代的收敛性   |
|      **SGD**       | 在线优化，连接 RM 与机器学习 |

---

### 10. Q&A

|         问题         |                             回答                             |
| :------------------: | :----------------------------------------------------------: |
| 为什么要学随机逼近？ |       **TD、Q-learning、Actor-Critic** 都可看作其特例        |
| 步长 $α_k$ 如何选？  | 常用 $\dfrac1k$ 或 常数衰减；需满足 $\sum_{k=1}^\infty α_k = ∞ \quad\sum_{k=1}^\infty α_k² < ∞\\$ |
|  SGD 与 RM 的关系？  |           **SGD** 是 **RM** 在优化问题上的直接应用           |
| 何时用 mini-batch？  |                  需降低方差又保持在线更新时                  |

<div style="page-break-after:always"></div>

## 第七章：时序差分方法

> **TD = 蒙特卡洛 + 动态规划：用“部分回报 + 旧估计”在线更新值函数，兼顾低方差与增量学习。**

---

### 1. TD 算法总览

|       算法       |            目标            |         策略类型         |          关键公式           |
| :--------------: | :------------------------: | :----------------------: | :-------------------------: |
| **TD(0) 状态值** | 估计给定策略的状态值 $v_π$ |          仅评估          |    $v ← v + α [r+γv'−v]$    |
|    **Sarsa**     | 估计给定策略的动作值 $q_π$ | 评估 + 改进 (on-policy)  |    $q ← q + α [r+γq'−q]$    |
| **n-step Sarsa** |  用 $n$ 步回报估计 $q_π$   |       评估 (通用)        |  $q ← q + α [G_t^{(n)}−q]$  |
|  **Q-learning**  |     直接估计最优 $q^*$     | 评估 + 改进 (off-policy) | $q ← q + α [r+γ \max q'−q]$ |

---

### 2. TD(0)：最简 TD 状态值更新

- **更新公式**  
  $$
  v_{t+1}(s_t) = v_t(s_t) + α_t [ r_{t+1} + γv_t(s_{t+1}) − v_t(s_t) ]
  $$
  
  - 仅更新**被访问状态**  
  - 收敛条件：$\sum_{k=1}^\infty α_k = ∞ \quad\sum_{k=1}^\infty α_k² < ∞\\$
  
- **TD 误差**  
  $$
  δ_t = [r_{t+1} + γv_t(s_{t+1})] − v_t(s_t)
  $$
  → 实时“创新”信号，校正当前估计。

---

### 3. Sarsa：在线策略动作值学习

- **更新公式**  
  $$
  q_{t+1}(s_t,a_t) = q_t + α_t [ r_{t+1} + γq_t(s_{t+1},a_{t+1}) − q_t ]
  $$
  
- **特点**  
  - **on-policy**：行为策略 $π_b$ 与目标策略 $π_T$ 相同  
  - 与策略改进结合：每步后把 $π$ 更新为 **ε-greedy(q)**

---

### 4. n-step Sarsa：TD 与 MC 的桥梁

- **n 步回报**  
  $$
  G_t^{(n)} = Σ_{k=1}^{n} γ^{k-1}r_{t+k} + γ^n q_t(s_{t+n},a_{t+n})
  $$
  
- **极端情况**  
  - $n=1$ $→$ 普通 **Sarsa**  
  - $n→∞ →$ **MC**（无自举，高方差）

- **偏差-方差权衡**：大 $n$ 降低偏差、提高方差。

---

### 5. Q-learning：off-policy 最优动作值学习

- **更新公式**  
  $$
  q_{t+1}(s_t,a_t) = q_t + α_t [ r_{t+1} + γ \max_{a'} q_t(s_{t+1},a') − q_t ]
  $$
  
- **特性**  
  - **off-policy**：行为策略 $π_b$ 与目标策略 $π_T$（贪心）**可不同**  
  - 直接逼近 **Bellman** 最优方程  
  - 可用任意行为策略收集数据（例如随机、**ϵ-greedy**、人类示范）

- **两种实现**  
  
  1. **在线**：行为策略 = **ε-greedy**  
  2. **离线**：先批量收集数据，再离线更新 $q$

---

### 6. on-policy vs off-policy 对比

|   维度   | on-policy (Sarsa) |  off-policy (Q-learning)   |
| :------: | :---------------: | :------------------------: |
| 行为策略 |  与目标策略一致   | 可任意（需覆盖所有 (s,a)） |
| 样本效率 |       较低        |    较高（可重用旧数据）    |
| 实现难度 |       简单        |       需处理分布差异       |
| 最终策略 |     ε-greedy      |            贪心            |

---

### 7. 统一视角

所有 **TD** 算法可写成  
$$
q ← q + α [ ¯q − q ]
$$
其中 $¯q$ 为不同 **TD** 目标：

|       算法       |                        $¯q$ 表达式                        |
| :--------------: | :-------------------------------------------------------: |
|    **Sarsa**     |                     $r + γ q(s',a')$                      |
| **n-step Sarsa** | $\sum_{k=1}^n γ^{k-1} r_{t+k} + γ^n q(s_{t+n},a_{t+n})\\$ |
|  **Q-learning**  |               $r + γ \max_{a'} q(s',a')\\$                |
|      **MC**      |             $\sum_{k=1}^∞ γ^{k-1} r_{t+k}\\$              |

---

### 8. 本章小结 

- **TD 家族** = 用增量方式求解 **Bellman** 方程  
- **Sarsa** 在线修正策略 $→$ 稳定但样本效率低  
- **Q-learning** 离线逼近最优 $→$ 样本复用高，但需保证充分探索  
- **n-step** 在偏差-方差之间滑动  
- 所有算法均属于广义策略迭代（**GPI**）：评估+改进交替进行。

---

### 9. Q&A

|               问题               |                         答案                         |
| :------------------------------: | :--------------------------------------------------: |
|   **TD** 与 **MC** 最大区别？    | **TD** 可增量更新、低方差；**MC** 需完整轨迹、高方差 |
|       为什么用常数 **α**？       |    策略非平稳，衰减 **α** 会过快收敛导致无法跟踪     |
| **Q-learning** 为何 off-policy？ |    直接逼近 **Bellman** 最优方程，不依赖特定策略     |
|    **ε-greedy** 何时可去掉？     |  训练后期可衰减到 $0$，或在离线阶段设 $π_T$ 为贪心   |

<div style="page-break-after:always"></div>

## 第八章：值函数方法

> **Value Function Methods** = 表格表示 → 函数表示：用参数化函数近似状态或动作值，解决大空间存储与泛化问题。

---

### 1. 值表示：从表格到函数

| 比较维度 |    表格表示     |               函数表示               |
| :------: | :-------------: | :----------------------------------: |
| 存储形式 |    数组/向量    |      参数向量 $ \mathbf{w} $       |
|  值获取  |    直接查表     | 前向计算 $ \hat{v}(s,\mathbf{w}) $ |
|  值更新  |    直接改写     |    通过梯度更新 $ \mathbf{w} $     |
| 泛化能力 |       无        |    有（更新一个状态影响其他状态）    |

---

### 2. 线性函数近似

- **形式**：$ \hat{v}(s,\mathbf{w}) = \phi(s)^\top\mathbf{w} $
- **特征向量**：$ \phi(s) \in \mathbb{R}^d $，需人工设计
- **示例**：
  - 多项式特征：$ [1,x,y,x^2,y^2,xy]^\top $
  - Fourier特征：$ [\cos(\pi(c_1x+c_2y))] $
  - Tile coding：离散化特征

---

### 3. TD学习：状态值估计

|     步骤     |                             内容                             |
| :----------: | :----------------------------------------------------------: |
| **目标函数** | $ J(\mathbf{w}) = \mathbb{E}_\mu[(v_\pi(s) - \hat{v}(s,\mathbf{w}))^2] $ |
| **梯度更新** | $ \mathbf{w}_{t+1} = \mathbf{w}_t + \alpha \delta_t \phi(s_t) $ |
|  **TD误差**  | $ \delta_t = r_{t+1} + \gamma \hat{v}(s_{t+1},\mathbf{w}_t) - \hat{v}(s_t,\mathbf{w}_t) $ |
|  **收敛性**  |      平稳分布 $ \mu $ 下收敛到投影 **Bellman** 方程的解      |

---

### 4. 最小二乘TD（LSTD）

- **思想**：直接解最小二乘，无需步长
- **计算**：
  - 构造 $ \hat{A}_t = \sum \phi_t(\phi_t - \gamma\phi_{t+1})^\top $
  - 构造 $ \hat{b}_t = \sum r_{t+1}\phi_t $
  - 解：$ \mathbf{w}_t = \hat{A}_t^{-1}\hat{b}_t $
- **优缺点**：
  - 优点：样本效率高
  - 缺点：需存储并求逆 $ d\times d $ 矩阵，复杂度 $ \mathcal{O}(d^3) $

---

### 5. Sarsa：动作值近似（on-policy）

- **Q近似**：$ \hat{q}(s,a,\mathbf{w}) = \phi(s,a)^\top\mathbf{w} $
- **更新**：
  $$
  \mathbf{w}_{t+1} = \mathbf{w}_t + \alpha \bigl[r_{t+1} + \gamma \hat{q}(s',a',\mathbf{w}_t) - \hat{q}(s,a,\mathbf{w}_t)\bigr]\phi(s,a)
  $$
- **策略改进**：每步后更新 $ \pi $ 为 **$ \varepsilon $-greedy**

---

### 6. Q-learning：动作值近似（off-policy）

- **更新**：
  $$
  \mathbf{w}_{t+1} = \mathbf{w}_t + \alpha \bigl[r_{t+1} + \gamma \max_{a'}\hat{q}(s',a',\mathbf{w}_t) - \hat{q}(s,a,\mathbf{w}_t)\bigr]\phi(s,a)
  $$
- **特性**：
  - 行为策略可任意
  - 直接逼近最优动作值 $ q^* $

---

### 7. 深度Q网络（DQN）

|    组件    |                             描述                             |
| :--------: | :----------------------------------------------------------: |
|  **网络**  |           深度神经网络 $ \hat{q}(s,a;\mathbf{w}) $           |
|  **目标**  |                 最小化 **Bellman** 最优误差                  |
| **目标值** |   $ y = r + \gamma \max_{a'}\hat{q}(s',a';\mathbf{w}^-) $    |
|  **技巧**  | 经验回放（**Experience Replay**）<br>目标网络（**Target Network**） |
|  **训练**  |                        小批量梯度下降                        |

---

### 8. 经验回放与目标网络

- **经验回放**：
  - 存储转移 $ (s,a,r,s') $ 于回放缓冲区
  - 随机采样小批量训练，打破相关性
- **目标网络**：
  - 参数 $ \mathbf{w}^- $ 每 $ C $ 步同步
  - 稳定训练，避免震荡

---

### 9. 本章小结

- **函数近似**解决大状态/动作空间问题
- **线性模型**理论清晰，特征设计关键
- **深度模型**自动特征，工程技巧重要
- **TD、Sarsa、Q-learning、DQN** 一脉相承，目标函数与更新方式递进

---

### 10. Q&A

| 问题                     | 答案                          |
| :----------------------: | :---------------------------: |
| 为何需要函数近似？       | 表格法存储与泛化不足          |
| 线性近似如何选特征？     | 多项式、**Fourier**、**Tile coding** |
| **LSTD** 与 **TD** 区别？ | **LSTD** 无需步长直接解最小二乘 |
| **DQN** 为何用目标网络？ | 固定 **TD** 目标，稳定训练    |
| 表格法是线性近似特例吗？ | 是，设 $ \phi(s)=e_s $ 即可 |

<div style="page-break-after:always"></div>

## 第九章：Policy Gradient Methods

> **Policy Gradient = 值函数方法 → 策略函数方法：用参数化策略 π(a|s,θ) 直接优化目标函数，实现策略空间的梯度上升。**

---

### 1. 策略表示：从表格到函数

| 比较维度 |   表格策略   | 函数策略（参数化） |
| :------: | :----------: | :----------------: |
| 表示形式 | π(a\|s) 查表 |   π(a\|s,θ) 函数   |
| 存储需求 |  O(∣S∣×∣A∣)  |     O(dim(θ))      |
| 更新方式 |   修改表项   |     梯度更新 θ     |
| 动作概率 |   直接读取   |   前向计算或采样   |
| 泛化能力 |      无      |   有（共享参数）   |

---

### 2. 优化目标（Metrics）

| 目标符号  |       定义        |   适用场景   |
| :-------: | :---------------: | :----------: |
|  **v̄_π**  |  Σ_s d(s) v_π(s)  | 折扣（γ<1）  |
| **v̄^0_π** | Σ_s d_0(s) v_π(s) | 起始分布独立 |
|  **r̄_π**  | Σ_s d_π(s) r_π(s) | 折扣/无折扣  |

- **等价关系**：折扣下 r̄_π = (1−γ)v̄_π
- **梯度目标**：最大化 J(θ) ∈ {v̄_π, v̄^0_π, r̄_π}

---

### 3. 策略梯度定理（Policy Gradient Theorem）

> **核心结论**：  
> $$
> ∇_θ J(θ) = E_{S∼η, A∼π} [ ∇_θ ln π(A|S,θ) q_π(S,A) ]
> $$
> 

- **η**：状态分布（d_π 或 ρ_π）
- **ln π**：确保正值、可微（常用 **Softmax**）
- **q_π**：动作价值（需估计）

---

### 4. 梯度推导：折扣 vs 无折扣

|       场景       |               关键公式               |       备注       |
| :--------------: | :----------------------------------: | :--------------: |
|  **折扣 $γ<1$**  | $∇_θ v̄_π ≈ Σ_s d_π(s) Σ_a ∇_θ π q_π$ |     近似成立     |
| **无折扣 $γ=1$** | $∇_θ r̄_π = Σ_s d_π(s) Σ_a ∇_θ π q_π$ |     严格成立     |
| **Poisson 方程** |    $v_π = r_π − r̄_π 1 + P_π v_π$     | 用于无折扣值定义 |

---

### 5. REINFORCE：蒙特卡洛策略梯度

- **算法步骤**  
  1. 按 $π(θ)$ 生成完整轨迹  
  
  2. 对每一步 $t$ 计算回报  
     $$
     G_t = Σ_{k=t+1}^T γ^{k-t-1} r_k
     $$
     
  3. 更新  
     $$
     θ ← θ + α ∇_θ \ln π(a_t|s_t,θ) G_t
     $$
     
  
- **特性**  
  - 无偏但高方差  
  - 需完整 **episode（MC）**  
  - 平衡探索/利用（ $β_t = G_t/π$ ）

---

### 6. 算法总结：REINFORCE 伪代码

> **算法 9.1：MC Policy Gradient (REINFORCE)**  
> **输入**：初始 θ，折扣 γ，学习率 α  
> **循环**  
> &nbsp;&nbsp;&nbsp;&nbsp;生成 episode {s_0,a_0,r_1,...,s_{T-1},a_{T-1},r_T}  
> &nbsp;&nbsp;&nbsp;&nbsp;**for** t = 0 … T−1 **do**  
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;G_t ← Σ_{k=t+1}^T γ^{k-t-1} r_k  
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;θ ← θ + α ∇_θ ln π(a_t|s_t,θ) G_t

---

### 7. 本章小结

- **策略梯度** = 直接优化策略参数 $θ$  
- **定理 9.1** 给出统一梯度形式  
- **REINFORCE** 是最简单实现，后续发展为 **Actor-Critic**  
- **折扣/无折扣** 场景需分别处理，但梯度结构一致

---

### 8. Q&A

|           问题           |                     答案                      |
| :----------------------: | :-------------------------------------------: |
| 策略梯度 vs 值函数方法？ |    前者直接优化策略，后者先求值再导出策略     |
|     $\ln π$ 的作用？     |       将梯度写成期望形式，便于随机近似        |
|  为何需要 **Softmax**？  |      保证 $π>0$ 且 $\sum_a π(a\|s)=1\\$       |
|    无折扣为何更简洁？    | 无需近似，梯度严格等于 $\sum d_π ∇_θ π q_π\\$ |
|   **REINFORCE** 缺点？   |    高方差、需完整 **episode**、样本效率低     |

<div style="page-break-after:always"></div>

## 第十章：Actor-Critic Methods

> **Actor-Critic = 策略梯度 + 值函数：Actor 负责策略更新，Critic 负责值估计，两者协作实现高效稳定的强化学习。**

---

### 1. Actor-Critic 基本结构

|    角色    |          功能           |          对应算法           |
| :--------: | :---------------------: | :-------------------------: |
| **Actor**  | 策略更新（参数 $$θ$$ ） |       $$π(a\|s, θ)$$        |
| **Critic** |  值估计（参数 $$w$$）   | $$q(s,a,w) $$ 或 $$v(s,w)$$ |

- **特征**：结合策略梯度（第9章）与值函数方法（第8章）
- **优势**：降低方差、提升样本效率、支持在线/离线策略

---

### 2. 最简单的 Actor-Critic：QAC

- **思想**：用 **TD-Sarsa** 估计 **q** 值，替代 **REINFORCE** 的 **MC** 估计
- **算法 ：QAC**
  
  1. **Actor**：
     $$
     θ ← θ + α_θ ∇_θ ln π(a_t\|s_t,θ) q(s_t,a_t,w)
     $$
     
  2. **Critic**：
     $$
     w ← w + α_w δ_t ∇_w q(s_t,a_t,w)，其中 δ_t = r + γ q(s_{t+1},a_{t+1},w) − q(s_t,a_t,w)
     $$
     

---

### 3. 优势 Actor-Critic：A2C

#### 3.1 基线不变性
- 对任意基线 b(s)：
  $$
  E[∇_θ \ln π q] = E[∇_θ \ln π (q − b)]
  $$
  
- 最优基线最小化方差：
  $$
  b^*(s) = E[∥∇_θ \ln π∥² q] / E[∥∇_θ \ln π∥²] ≈ v_π(s)
  $$
  

#### 3.2 A2C 算法 10.2
- **优势函数**：
  $$
  δ_t = r + γ v(s_{t+1},w) − v(s_t,w) ≈ q − v
  $$
  
- **Actor**：
  $$
  θ ← θ + α_θ δ_t ∇_θ \ln π(a_t\|s_t,θ)
  $$
  
- **Critic**：
  $$
  w ← w + α_w δ_t ∇_w v(s_t,w)
  $$
  
- **特点**：单网络（仅 **v**），**TD-error** 作为优势估计，天然探索

---

### 4. 离线策略 Actor-Critic

#### 4.1 重要性采样
- 目标：用行为策略 β 的样本优化目标策略 π

- **重要性权重**：
  $$
  w_t = π(a_t\|s_t,θ)/β(a_t\|s_t)
  $$
  

#### 4.2 离线策略 A2C 算法 10.3
- **Actor**：
  $$
  θ ← θ + α_θ w_t δ_t ∇_θ ln π(a_t\|s_t,θ)
  $$
  
- **Critic**：
  $$
  w ← w + α_w w_t δ_t ∇_w v(s_t,w)
  $$
  
- **要求**：β 的支撑需覆盖 π（β>0 当 π>0）

---

### 5. 确定性 Actor-Critic（DDPG 前身）

#### 5.1 确定性策略梯度定理

- **策略**：$a = μ(s,θ)$

- **梯度**（折扣/无折扣通用）：
  $$
  ∇_θ J = E_{s∼ρ} [ ∇_θ μ(s) ∇_a q(s,a) |_{a=μ(s)} ]
  $$

#### 5.2 确定性 AC 算法

- **Actor**：
  $$
  θ ← θ + α_θ ∇_θ μ(s_t) ∇_a q(s_t,a,w) |_{a=μ(s_t)}
  $$
  
- **Critic**：
  $$
  w ← w + α_w [r + γ q(s_{t+1},μ(s_{t+1},θ),w) − q(s_t,a_t,w)] ∇_w q(s_t,a_t,w)
  $$
  
- **行为策略**：$$β$$ 可为带噪声的 $$μ$$，实现高效探索

---

### 6. 算法演进路线

|     算法      |  策略  |  值估计   | 方差 | 样本效率 |        备注         |
| :-----------: | :----: | :-------: | :--: | :------: | :-----------------: |
| **REINFORCE** |  随机  |    MC     |  高  |    低    |   需完整 episode    |
|    **QAC**    |  随机  | TD-Sarsa  |  中  |    中    |      在线策略       |
|    **A2C**    |  随机  | TD + 基线 |  低  |    高    |       单网络        |
| **离线 A2C**  |  随机  |  TD + IS  |  低  |    高    |    重放任意数据     |
| **确定性 AC** | 确定性 |    TD     |  低  |    高    | 连续动作、DDPG 基础 |

---

### 7. 本章小结

- **Actor-Critic** 统一了策略梯度与值函数方法
- **基线**降低方差，**重要性采样**支持离线策略
- **确定性策略**天然离线，适合连续动作空间
- 为后续算法（**DDPG、TD3、SAC、PPO** 等）奠定理论基础

---

### 8. Q&A

|                      问题                      |                             答案                             |
| :--------------------------------------------: | :----------------------------------------------------------: |
| **Actor-Critic** 与 **Policy Gradient** 关系？ | **Actor-Critic** 是 **Policy Gradient** 的特例，用 **TD** 估计动作值 |
|                 为何引入基线？                 |                 保证无偏且最小化梯度估计方差                 |
|           重要性采样能否用于值函数？           |                  可以，任何期望估计均可使用                  |
|              确定性策略为何离线？              |             梯度不含动作随机变量，无需按策略采样             |
