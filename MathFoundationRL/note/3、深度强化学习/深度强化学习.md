# 深度强化学习

## 1、DQN

> DQN（深度 Q 网络）是使用神经网络来近似 Q 函数 $q(s,a)$ ，代替原本的表格（每个状态下的所有动作值）
>
> :star: 解决了过大的空间（如图像）所带来的维度灾难问题，但只能适用于离散动作空间

### :one: 核心原理

#### 神经网络近似 Q 函数

传统 Q-Learning 用表格存储 $q(s,a)$，DQN 用深度神经网络 $q(s,a;\theta)$ 近似，其中 $\theta$ 是网络参数

- 输入：状态 $s$（如游戏画面、传感器数据）
- 输出：每个动作 $a$ 对应的 Q 值（动作价值）

> [!note]
>
> 显然，这种函数拟合的方法存在一定的精度损失，因此被称为近似方法

##### 损失函数设计

DQN 的训练目标是最小化 “预测 Q 值” 与 “目标 Q 值” 的均方误差（MSE），损失函数（TD-error）定义为：

$$
\begin{align}
\mathcal{L}(\theta) 
&= \mathbb{E}_{(s,a,r,s',d)\in D}\left[ \left( y_{target} - q(s,a;\theta) \right)^2 \right]\\
&= \dfrac{1}{2N}\sum_{i=1}^N\bigg[r_i + (1-d)\gamma \max_{a} q(s_{i+1},a;\theta^-) -   q(s_i,a_i;\theta^-)   \bigg]
\end{align}
$$


#### 经验回放缓冲区（Replay Buffer）

存储智能体与环境交互产生的样本 $(s_t,a_t,r_{t+1},s_{t+1},d_{t+1})$

- $d$ 为终止标志，$d=1$ 表示状态 $s_{t+1}$ 是终止状态

**优势**：

1. 打破样本的时序相关性，使训练数据满足独立同分布假设，从而缓解分布偏移，稳定梯度更新
2. 样本可重复利用，提高数据利用率（原本 Q-learning 只使用一次数据，用完即弃）

> [!important]
>
> 深度学习模型的训练基于 “**数据独立同分布（i.i.d.）**” 假设
>
> - 训练样本之间相互独立，且服从相同的概率分布
>
> 因为神经网络通过梯度下降最小化损失函数时，每次更新参数都依赖于==当前批次样本的损失梯度==
>
> - 若样本独立，每次梯度更新的方向能更准确地指向 “全局最优”
>
> - 若样本存在相关性，梯度方向会被冗余信息干扰，导致参数震荡
>   - 强化学习中，agent 与环境交互产生的样本天然是时序关联的
>   - 相邻样本的损失梯度方向高度相似，导致梯度更新会有单向偏移的可能
>   - 进而导致参数在该方向上过度更新，且后续反向修正困难（震荡）
>   - 此外还会因为在各个阶段过拟合（分布偏移），导致泛化能力大幅下降

**关键参数**


  * 缓冲区容量 $N$

    * 通常设为 $10^5$\~$10^6$，避免存储过旧样本

  * 批量大小 $B$

    * 训练时每次从缓冲区随机采样 $B$ 个样本，通常取 32、64




#### 目标网络

固定目标值 $y_{target}$ 的计算基准，避免因在线网络参数波动导致目标值剧烈变化，稳定训练过程

> [!important]
>
> DQN 的 Q 网络的训练目标是
> $$
> q(s,a;\theta)\rarr r+\gamma\max_{a}q(s,a;\theta)
> $$
>
> - 因为二者均与自身输出有关，导致更新方向不断变化
>
> 为了使训练稳定有效，故创建一个新的网络，称为目标网络
>
> - 不实时更新，而是间歇性更新，保证一段时间内的训练稳定

**更新机制**

- 目标网络的参数 $\theta^-$ 不随在线网络实时更新
- 而是每隔 $C$ 步（通常 $C=1000$\~$10000$），将在线网络的参数 $\theta$ 复制给目标网络：$\theta^- \leftarrow \theta$



### :two: 算法流程

> [!note]
>
> 超参数：
>
> 学习率 $\alpha$、折扣因子 $\gamma$、目标网络更新间隔 $C$、容量大小 $N$、批量大小 $B$、探索率 $\epsilon$

<img src="./深度强化学习.assets/v2-50f046b4a92e1cc2bf79f9f723cc2579_1440w.jpg" alt="img" style="zoom:100%;" />



### :three: 主要代码

构建 `ReplayBuffer`

```python
import collections
import random
import numpy as np

class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = collections.deque(maxlen=capacity)
        
    def add(self, state, action, reward, next_state, done):
        self.buffer.append([state, action, reward, next_state, done])
        
    def sample(self, batch_size):
        transitions = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = zip(*transitions)
        return np.array(state), action, reward, np.array(next_state), done
    
    def size(self):
        return len(self.buffer)
        
```

定义 `Q-Network`

```python
import torch
import torch.nn.functional as F

class Qnet(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(Qnet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        return self.fc2(x)
```

定义 `Agent`

```python
class DQN:
    def __init__(self, state_dim, hidden_dim, action_dim, lr, gamma, epsilon, C, device):
        self.action_dim = action_dim
        
        self.q_net = Qnet(state_dim, hidden_dim, action_dim).to(device)
        self.target_q_net = Qnet(state_dim, hidden_dim, action_dim).to(device)
        
        self.optimize = torch.optim.Adam(self.q_net.parameters(), lr=lr)
        self.gamma = gamma
        self.epsilon = epsilon
        self.C = C
        self.count = 0
        self.device = device
        
    def take_action(self, state):
        if torch.rand(1).item() < self.epsilon:
            action = np.random.randint(self.action_dim)
        else:
            state = torch.tensor([state], dtype=torch.float).to(self.device)
            action = self.q_net(state).argmax().item()
        return action
    
	def update(self, transition_dict):
        states = torch.tensor(transition_dict['states'],
                              dtype=torch.float).to(self.device)
        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(
            self.device)
        rewards = torch.tensor(transition_dict['rewards'],
                               dtype=torch.float).view(-1, 1).to(self.device)
        next_states = torch.tensor(transition_dict['next_states'],
                                   dtype=torch.float).to(self.device)
        dones = torch.tensor(transition_dict['dones'],
                             dtype=torch.float).view(-1, 1).to(self.device)

        q_values = self.q_net(states).gather(1, actions)
        max_next_q_values = self.target_q_net(next_states).max(1)[0].view(-1, 1)
        q_targets = rewards + self.gamma * max_next_q_values * (1 - dones)
        dqn_loss = torch.mean(F.mse_loss(q_values, q_targets))
        
        self.optimizer.zero_grad()
        dqn_loss.backward()
        self.optimizer.step()

        if self.count % self.C == 0:
            self.target_q_net.load_state_dict(self.q_net.state_dict())
        self.count += 1
```



### :four: Double DQN

> 目标在于解决==最大值偏差问题==

DQN 中，目标 Q 值 $y_{target} = r + \gamma \max_{a'} q(s',a';\theta^-)$ 用同一目标网络既选动作（$\max_{a'}$）又估价值

- 可能导致对 Q 值的系统性高估（最大值偏差）

**核心改进**

用在线网络选动作，目标网络估价值，拆分 “选动作” 和 “估价值” 的过程
$$
y_{target} = r + \gamma \cdot q\left( s', \arg\max_{a'} q(s',a';\theta); \theta^- \right)
$$

* 有效缓解最大值偏差，使 Q 值估计更准确，训练更稳定
* 道理和 Q-learning 中的 双Q 相同



### :five: Dueling DQN

> 提出优势函数 $A(s,a)=Q(s,a)-V(s)$，表示采取不同动作的差异性

将 Q 函数拆分为 状态价值 $v(s)$  和 动作优势 $a(s,a)$
$$
q(s,a;\theta_\eta,\theta_\alpha,\theta_\beta)=v(s;\theta_\eta,\theta_\alpha)+a(s,a;\theta_\eta,\theta_\beta)
$$

* 状态价值 $v(s;\theta_\eta,\theta_\alpha)$：表示状态 $s$ 本身的价值（与动作无关）

* 动作优势 $a(s,a;\theta_\eta,\theta_\beta)$：表示动作 $a$ 相对于其他动作的优势（动作间的差异）

* $\theta_\eta$ 为状态价值函数和优势函数共享的网络参数，一般用在神经网络中，用来提取特征的前几层

> [!important]
>
> 将状态价值函数和优势函数分别建模的好处在于
>
> - 某些情境下智能体只会关注状态的价值，而并不关心不同动作导致的差异，做什么动作都差不多结果
> - 某些情境下智能体会开始关心不同动作导致的差异，因为特定的动作能带来显著的优势
> - 此时将二者分开建模能够使 agent 更好地处理与动作关联较小的状态
>
> 每一次更新时， $v(s;\theta_\eta,\theta_\alpha)$ 都会被更新，这也会影响到其他动作的 Q 值
>
> 传统的 DQN 只会更新某个动作的 Q 值，其他动作的 Q 值就不会更新
>
> 因此，Dueling DQN 能够更加频繁、准确地学习状态价值函数

> [!caution]
>
> 上述公式存在建模不唯一性的问题（参数冗余），会导致训练不稳定（不同的 $v、a$ 可能可以得到相同的 Q 值）

故理论上应减去最大优势
$$
q(s,a;\theta_\eta,\theta_\alpha,\theta_\beta)=v(s;\theta_\eta,\theta_\alpha)+a(s,a;\theta_\eta,\theta_\beta)- \max_{a'} a(s,a';\theta_\eta,\theta_\beta)
$$
实际工程会使用平均替代最大化操作
$$
q(s,a;\theta_\eta,\theta_\alpha,\theta_\beta)=v(s;\theta_\eta,\theta_\alpha)+a(s,a;\theta_\eta,\theta_\beta)- \frac{1}{|A|}\sum_{a'} a(s,a';\theta_\eta,\theta_\beta)
$$

#### 一、减去最大值能确保建模唯一性

- 对于任意动作 $a^* = \arg\max_{a'} A(s,a')$，其对应的 Q 值
  $$
  Q(s,a^*) = V(s) + A(s,a^*) - A(s,a^*) = V(s)
  $$

- $V(s)$ 的定义是 “状态 s 下所有动作的最大 Q 值”，因此 $V(s)$ 必须等于该状态下最优动作的 Q 值

- $A(s,a)$ 表示 “动作 a 相对于最优动作的优势”，其取值也被唯一约束（因为 $\max_{a'} A(s,a') = 0$）

#### 二、最大值可以替换为均值

用**均值**替代最大值的核心原因是**降低计算复杂度并提升稳定性**：

- 最大值操作是**非光滑的**
  - 当多个动作的 $A$ 值接近最大值时，微小的参数变化会导致最大值的 “跳跃”
  - 这会给梯度计算带来噪声，影响训练稳定性

- 均值操作是**光滑的**
  - 对 $A$ 网络的参数变化更鲁棒，梯度更新更稳定，在工程实践中更容易收敛

从直觉上，均值可以理解为动作优势的平均基准

- 动作 a 的优势是相对于所有动作的平均水平，而非仅相对于最优动作，这种定义更平滑

#### 三、替换为均值后偏离贝尔曼最优方程

贝尔曼最优方程的核心是
$$
Q^*(s,a) = r + \gamma \max_{a'} Q^*(s',a')
$$

- 它要求 Q 值的更新严格基于下一状态的最大 Q 值

用均值修正后，对应的 Q 值更新逻辑变成
$$
Q(s,a) \leftarrow r + \gamma \cdot \frac{1}{|A|}\sum_{a'} Q(s',a')
$$
这是理论最优与实践可行的权衡 

- 虽然偏离了贝尔曼最优方程，但均值修正让 $ V$ 和 $A$ 的建模更唯一、梯度更新更稳定
  - 模型能在训练中收敛到一个**近似最优**的解。
- 在很多实际任务中，平均信息已经能足够刻画状态价值 
  - 状态的价值往往和多数动作的平均表现强相关，不一定需要严格依赖最优动作的信息

### :six: Prioritized Experience Replay

> 优先采样重要样本（PER），提出考虑样本的 “重要性”（如误差大的样本对参数更新贡献更大）
>
> 聚焦于高价值样本（误差大、信息量大），加速训练收敛，提高样本利用效率

根据样本的 TD 误差 $\delta = |y_{target} - q(s,a;\theta)|$ 分配采样优先级，误差越大的样本被采样的概率越高

- 优先级计算
  $$
  p_i = (\delta_i + \epsilon)^\alpha
  $$

  - $\epsilon$ 避免优先级为 0
  - $\alpha$ 控制优先级权重，$\alpha=0$ 时退化为随机采样

- 采样概率
  $$
  P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}
  $$


* 权重修正


  * 为避免优先级采样导致的偏差，对梯度更新加入权重因子 
    $$
    w_i = \left( \frac{1}{N} \cdot \frac{1}{P(i)} \right)^\beta
    $$

    - $\beta$ 为平衡偏差，训练中逐渐从 0.4 增加到 1.0

* 损失函数
  $$
  \mathcal{L}(\theta) = \frac{1}{2N} \sum_{i=1}^{N} \dfrac{w_i}{\max (w)} \left( y_i - q(s_i, a_i; \theta) \right)^2
  $$

  - 权重需要归一化



## 2、策略梯度（Policy Gradient, PG）

> 之前的 Q-learning、DQN 算法都是**基于价值**（value-based）的方法，学习值函数，然后根据值函数导出一个策略
>
> **基于策略**（policy-based）的方法则是直接显式地学习一个目标策略
>
> :star: 策略梯度直接参数化策略 $\pi(a|s;\theta)$，通过梯度上升最大化累积奖励的期望，适用于离散和连续动作空间

### :one: 核心原理

#### 策略参数化

策略梯度直接对策略进行建模，用神经网络拟合在状态 $s$ 下采取动作 $a$ 的概率分布

- 离散动作空间（类别分布，经 `softmax` 激活）
  $$
  \pi(a|s;\theta) = P(A=a|S=s;\theta)
  $$

- 连续动作空间：$\pi(a|s;\theta)$ 为概率密度函数

  - 如高斯分布，均值和方差由神经网络输出

#### 目标函数

策略梯度的目标是最大化**累积奖励的期望**
$$
J(\theta) = \mathbb{E}_{s_0} \left[ V_{\pi_\theta}(s_0) \right]
$$

- 其中：$s_0$ 表示初始状态

#### 策略梯度定理

目标函数关于参数 $\theta$ 的梯度可表示为
$$
\begin{align}
\nabla _\theta J(\theta) 
&\propto \sum_{s\in S}\nu_{\pi_\theta}(s)\sum_{a\in A}Q_{\pi_\theta}(s,a)\nabla_\theta\pi_\theta(a|s)\\
&=\sum_{s\in S}\nu_{\pi_\theta}(s)\sum_{a\in A}\pi_\theta(a|s)Q_{\pi_\theta}(s,a)\dfrac{\nabla_\theta\pi_\theta(a|s)}{\pi_\theta(a|s)}\\
&=\mathbb E_{\pi_\theta}\left[Q_{\pi_\theta}(s,a)\nabla_\theta\log{\pi_\theta(a|s)}\right]
\end{align}
$$

- 其中：$\nu_{\pi_\theta}(s)$ 为**策略的状态访问分布**，表示策略 $\pi_\theta$ 下状态 $s$​ 被访问的加权频率
  $$
  \nu_\pi(s)=(1-\gamma)\sum_{t=0}^\infty\gamma^tP_{\pi_\theta}(s)
  $$

> [!warning]
>
> 策略梯度算法为在线策略（on-policy）算法，即必须使用当前策略 $\pi_\theta$ 采样得到的数据来计算梯度

##### 一、状态价值函数的梯度 $\nabla_\theta V_{\pi_\theta}(s)$ 推导 

状态价值函数定义为策略 $\pi_\theta$ 下状态 $s$ 的各动作价值期望
$$
V_{\pi_\theta}(s) = \sum_{a \in A} \pi_\theta(a|s) Q_{\pi_\theta}(s,a)
$$
对其关于策略参数 $\theta$ 求梯度
$$
\begin{align*} 
\nabla_\theta V_{\pi_\theta}(s) 
&= \nabla_\theta \left( \sum_{a \in A} \pi_\theta(a|s) Q_{\pi_\theta}(s,a) \right) \\ 
&= \sum_{a \in A} \left( \nabla_\theta \pi_\theta(a|s) \cdot Q_{\pi_\theta}(s,a) + \pi_\theta(a|s) \cdot \nabla_\theta Q_{\pi_\theta}(s,a) \right)  \\ 
\end{align*}
$$
接下来处理  $\nabla_\theta Q^{\pi_\theta}(s,a)$

动作价值函数的定义是
$$
Q_{\pi_\theta}(s,a) = \mathbb{E} \left[ r + \gamma V_{\pi_\theta}(s') | s,a \right] = \sum_{s',r} p(s',r|s,a) \left( r + \gamma V_{\pi_\theta}(s') \right)
$$
对其关于策略参数 $\theta$ 求梯度
$$
\nabla_\theta Q_{\pi_\theta}(s,a) = \sum_{s',r} p(s',r|s,a) \gamma \nabla_\theta V_{\pi_\theta}(s')
$$
将其代回上式
$$
\begin{align*} 
\nabla_\theta V_{\pi_\theta}(s) 
&= \sum_{a \in A} \nabla_\theta \pi_\theta(a|s) Q_{\pi_\theta}(s,a) + \sum_{a \in A} \pi_\theta(a|s) \cdot \sum_{s'} p(s'|s,a) \gamma \nabla_\theta V_{\pi_\theta}(s') \\ 
&= \sum_{a \in A} \nabla_\theta \pi_\theta(a|s) Q_{\pi_\theta}(s,a) + \gamma \sum_{s'} \left( \sum_{a \in A} \pi_\theta(a|s) p(s'|s,a) \right) \nabla_\theta V_{\pi_\theta}(s') \\ \end{align*}
$$
定义简化符号
$$
\phi(s) = \sum_{a \in A} \nabla_\theta \pi_\theta(a|s) Q_{\pi_\theta}(s,a)\\
P_{\pi_\theta}(s'|s) = \sum_{a \in A} \pi_\theta(a|s) p(s'|s,a)
$$
则上式可写为
$$
\nabla_\theta V_{\pi_\theta}(s) = \phi(s) + \gamma \sum_{s'} P_{\pi_\theta}(s'|s) \nabla_\theta V_{\pi_\theta}(s')
$$

###### 递推展开与收敛（几何级数求和）

将上式反复递推展开（代入自身）
$$
\nabla_\theta V_{\pi_\theta}(s) = \phi(s) + \gamma \sum_{s'} P_{\pi_\theta}(s'|s) \phi(s') + \gamma^2 \sum_{s''} P_{\pi_\theta}(s''|s') P_{\pi_\theta}(s'|s) \nabla_\theta V_{\pi_\theta}(s'')
$$
以此类推，最终得到**无穷级数展开**
$$
\nabla_\theta V_{\pi_\theta}(s) = \sum_{k=0}^\infty \gamma^k \sum_{x \in S} d_{\pi_\theta}(s \to x, k) \phi(x)
$$

- 其中 $d_{\pi_\theta}(s \to x, k)$ 是策略 $\pi_\theta$ 从 $s$ 出发 $k$ 步后到达 $x$ 的概率（即 k 步转移概率的累积）

##### 二、目标函数的梯度 $\nabla_\theta J(\theta)$ 推导

目标函数定义为初始状态的期望价值
$$
J(\theta) = \mathbb{E}_{s_0} \left[ V_{\pi_\theta}(s_0) \right]
$$
对其关于策略参数 $\theta$ 求梯度
$$
\nabla_\theta J(\theta) = \mathbb{E}_{s_0} \left[ \nabla_\theta V_{\pi_\theta}(s_0) \right]
$$
将 $\nabla_\theta V_{\pi_\theta}(s_0)$ 代入
$$
\begin{align*} 
\nabla_\theta J(\theta) 
&= \mathbb{E}_{s_0} \left[ \sum_{k=0}^\infty \gamma^k \sum_{x \in S} d_{\pi_\theta}(s_0 \to x, k) \phi(x) \right] \\ 
&= \sum_{x \in S} \phi(x) \cdot \mathbb{E}_{s_0} \left[ \sum_{k=0}^\infty \gamma^k d_{\pi_\theta}(s_0 \to x, k) \right] \\ 
\end{align*}
$$
定义简化符号
$$
\eta_{\pi_\theta}(x) = \mathbb{E}_{s_0} \left[ \sum_{k=0}^\infty \gamma^k d_{\pi_\theta}(s_0 \to x, k) \right]
$$
则
$$
\nabla_\theta J(\theta) = \sum_{s \in S} \eta_{\pi_\theta}(s) \cdot \phi(s)
$$
又因为：
$$
\sum_{s \in S} \eta_{\pi_\theta}(s)=\left(\sum_{s \in S} \eta_{\pi_\theta}(s)\right)\sum_s\dfrac{\eta_{\pi_\theta}(s)}{\sum_{s \in S} \eta_{\pi_\theta}(s)}\propto \sum_s\dfrac{\eta_{\pi_\theta}(s)}{\sum_{s \in S} \eta_{\pi_\theta}(s)}=\sum_{s \in S}\nu_{\pi_\theta}
$$
最终得到
$$
\nabla_\theta J(\theta) \propto \sum_{s \in S} \nu_{\pi_\theta}(s) \sum_{a \in A} Q_{\pi_\theta}(s,a) \nabla_\theta \pi_\theta(a|s)
$$

### :two: REINFORCE 

REINFORCE 是最简单的策略梯度算法，使用 MC 方法来估计 $Q_{\pi_\theta}(s,a)$​，基于完整轨迹更新参数

- 无偏、但大方差的梯度估计

$$
\begin{align}
\nabla_\theta J(\theta) 
&\propto \mathbb{E}_{\pi_\theta} \left[ \sum_{t=0}^{T-1}G_t \nabla_\theta \log \pi_\theta(a_t|s_t)  \right]\\
&=\mathbb{E}_{\pi_\theta} \left[ \sum_{t=0}^{T-1}\big(\sum_{k=t}^{T-1} \gamma^{k-t} r_{k}\big) \nabla_\theta \log \pi_\theta(a_t|s_t)  \right]\\
\end{align}
$$

####  主要代码

创建 `PolicyNet`

```python
import torch
import torch.nn.functional as F

class PolicyNet(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(PolicyNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        # 在离散动作空间上的softmax()函数来实现一个可学习的多项分布（multinomial distribution）
        return F.softmax(self.fc2(x), dim=1)
```

创建 `agent`

```python
class REINFORCE:
    def __init__(self, state_dim, hidden_dim, action_dim, learning_rate, gamma, device):
        self.policy_net = PolicyNet(state_dim, hidden_dim, action_dim).to(device)
        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=learning_rate)
        self.gamma = gamma
        self.device = device
	
    # 根据动作概率分布随机采样
    def take_action(self, state):  
        state = torch.tensor([state], dtype=torch.float).to(self.device)
        probs = self.policy_net(state)
        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()
        return action.item()

    def update(self, transition_dict):
        reward_list = transition_dict['rewards']
        state_list = transition_dict['states']
        action_list = transition_dict['actions']

        G = 0
        self.optimizer.zero_grad()
        # 从最后一步算起
        for i in reversed(range(len(reward_list))):  
            reward = reward_list[i]
            state = torch.tensor([state_list[i]], 
                                 dtype=torch.float).to(self.device)
            action = torch.tensor([action_list[i]], 
                                  dtype=torch.float).view(-1, 1).to(self.device)
            # 计算策略的对数概率
            probs = self.policy_net(state)
            dist = torch.distributions.Categorical(probs)
			dist_log_prob = dist.log_prob(action)
            # dist_log_prob = torch.log(self.policy_net(state).gather(1, action))
            G = self.gamma * G + reward
            loss = -dist_log_prob * G
            # 反向传播计算梯度
            loss.backward()  
        # 梯度下降
        self.optimizer.step()  
```

> [!note]
>
> 在函数`take_action()`函数中，通过动作概率分布对离散的动作进行采样
>
> 在更新过程中，我们按照算法将损失函数写为策略回报的负数，求导后就可以通过梯度下降来更新策略



> [!important]
>
> 除了使用蒙特卡洛估计，还有其他方法可以估计 $Q_{\pi_\theta}(s,a)$，从而衍生出多种算法
>
> 1. 轨迹总回报
>    $$
>    \sum_{t'=0}^{T-1}\gamma^{t'}r_t'
>    $$
>
> 2. 动作 $a_t$ 之后的回报
>    $$
>    \sum_{t'=t}^{T-1}\gamma^{t'-t}r_{t'}
>    $$
>
> 3. 使用基准线改进（减小方差）
>    $$
>    \sum_{t'=t}^{T-1}\gamma^{t'-t}r_{t'}-b(s_t)
>    $$
>
> 4. 神经网络估计动作价值函数（Actor-Critic）（减小方差）
>    $$
>    Q_{\pi_\theta}(s,a；\omega)
>    $$
>
> 5. 优势函数（带基准线 $v$ 的 Actor-Critic）（减小方差）
>    $$
>    A_{\pi_\theta}(s,a；\omega)
>    $$
>
> 6. TD-error（带基准线 $v$ 且 bootstrap 的 Actor-Critic）（减小方差）
>    $$
>    r_t+\gamma v_{\pi_\theta}(s_{t+1})-v_{\pi_\theta}(s_t)
>    $$



### :three: Actor-Critic 

> :star: 结合策略梯度（Actor）和价值函数（Critic），并使用 TD-error 实现单步更新，以提高样本效率：
>
> - **Actor**：策略网络 $\pi(a|s;\theta)$，负责输出动作
> - **Critic**：价值网络 $Q(s,a; \phi)$ 或 $V(s; \phi)$，负责评估动作价值

#### 核心思想

Actor 要做的是与环境交互

- 在 Critic 价值函数的指导下用策略梯度学习一个更好的策略

Critic 要做的是通过 Actor 与环境交互收集的数据学习一个价值函数

- 这个价值函数会用于判断在当前状态什么动作是好的，什么动作不是好的，进而帮助 Actor 进行策略更新

#### 损失函数

$$
L(\omega)=\dfrac12\left(r_t+\gamma v(s_{t+1};\omega) -v(s_t;\omega)\right)^2
$$

#### 梯度

$$
\nabla_\omega L(\omega)=-(r+\gamma v(s_{t+1};\omega) -v(s_t;\omega))\nabla_\omega v(s_t;\omega)
$$

####  主要代码

创建 `PolicyNet`

```python
import torch
import torch.nn.functional as F

class PolicyNet(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(PolicyNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        # 在离散动作空间上的softmax()函数来实现一个可学习的多项分布（multinomial distribution）
        return F.softmax(self.fc2(x), dim=1)
```

创建 `ValueNet`

```python
class ValueNet(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim):
        super(ValueNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return self.fc2(x)
```

创建 `agent`

```python
class ActorCritic:
    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr, gamma, device):
        # 策略网络
        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)
        # 价值网络
        self.critic = ValueNet(state_dim, hidden_dim).to(device)  
        # 策略网络优化器
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)
     	# 价值网络优化器
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr) 
        self.gamma = gamma
        self.device = device

    def take_action(self, state):
        state = torch.tensor([state], dtype=torch.float).to(self.device)
        probs = self.actor(state)
        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()
        return action.item()

    def update(self, transition_dict):
        states = torch.tensor(transition_dict['states'],
                              dtype=torch.float).to(self.device)
        actions = torch.tensor(transition_dict['actions'], 
                               dtype=torch.float).view(-1, 1).to(self.device)
        rewards = torch.tensor(transition_dict['rewards'], 
                               dtype=torch.float).view(-1, 1).to(self.device)
        next_states = torch.tensor(transition_dict['next_states'], 
                                   dtype=torch.float).to(self.device)
        dones = torch.tensor(transition_dict['dones'],
                             dtype=torch.float).view(-1, 1).to(self.device)
        # 时序差分目标
        td_target = rewards + self.gamma * self.critic(next_states) * (1 - dones)
        # 时序差分误差
        td_delta = td_target - self.critic(states)  
        log_probs = torch.log(self.actor(states).gather(1, actions))
        actor_loss = torch.mean(-log_probs * td_delta.detach())
        # 均方误差损失函数
        critic_loss = torch.mean(F.mse_loss(self.critic(states), td_target.detach()))
        self.actor_optimizer.zero_grad()
        self.critic_optimizer.zero_grad()
        # 计算策略网络的梯度
        actor_loss.backward()  
        # 计算价值网络的梯度
        critic_loss.backward() 
        # 更新策略网络的参数
        self.actor_optimizer.step() 
        # 更新价值网络的参数
        self.critic_optimizer.step()  
```



### :four: TRPO 

> 当策略网络是深度模型时，沿着策略梯度更新参数，很有可能由于步长太长，策略突然显著变差，进而影响训练效果
>
> :star:**信任区域策略优化**（trust region policy optimization）算法引入了**信任区域**，能够保证策略学习的性能单调性

#### 策略目标

假设当前策略为 $\pi_\theta$ ，算法的目标是借助当前的 $\theta$，来找到一个更优的 $\theta'$，使得 $J(\theta')\geq J(\theta)$

已知
$$
\begin{align}
J(\theta)
&=\mathbb E_{s_0}\left[v_{\pi_\theta}(s_0)\right]\\
&=\mathbb E_{\pi_{\theta'}}\left[\sum_{t=0}^\infty\gamma^tv_{\pi_\theta}(s_t)-\sum_{t=1}^\infty\gamma^tv_{\pi_\theta}(s_t) \right]\\
&=-\mathbb E_{\pi_{\theta'}}\left[\sum_{t=0}^\infty\gamma^t\left(\gamma v_{\pi_\theta}(s_{t+1})-v_{\pi_\theta}(s_t)\right) \right]\\
\end{align}
$$
可以得到新旧策略的目标函数 $J(\theta)$ 的差距
$$
\begin{align}
J(\theta')-J(\theta)
&=\mathbb E_{s_0}\left[v_{\pi_{\theta'}}(s_0)\right]-\mathbb E_{s_0}\left[v_{\pi_\theta}(s_0)\right]\\
&=\mathbb E_{\pi_{\theta'}}\left[\sum_{t=0}^\infty\gamma^tr(s_t,a_t)\right]+\mathbb E_{\pi_{\theta'}}\left[\sum_{t=0}^\infty\gamma^tv_{\pi_\theta}(s_t)-\sum_{t=1}^\infty\gamma^tv_{\pi_\theta}(s_t) \right]\\
&=\mathbb E_{\pi_{\theta'}}\left[\sum_{t=0}^\infty\gamma^t\left[r(s_t, a_t)+\gamma v_{\pi_\theta}(s_{t+1})-v_{\pi_\theta}(s_t)\right] \right]\\
\end{align}
$$
其中 $r(s_t, a_t)+\gamma v_{\pi_\theta}(s_{t+1})-v_{\pi_\theta}(s_t)$  是 TD-error，定义其为优势函数 $A_{\pi_\theta}(s_t,a_t)$
$$
\begin{align}
J(\theta')-J(\theta)
&=\mathbb E_{\pi_{\theta'}}\left[\sum_{t=0}^\infty\gamma^tA_{\pi_\theta}(s_t, a_t) \right]\\
&=\sum_{t=0}^\infty\gamma^t\mathbb E_{s_t\sim P_{\pi_\theta(\cdot|s_t)}}\mathbb E_{a_t\sim\pi_{\theta'}(\cdot|s_t)}\left[A_{\pi_\theta}(s_t, a_t) \right]\\
&=\dfrac{1}{1-\gamma}\mathbb E_{s\sim\nu_{\pi_{\theta'}}}\mathbb E_{a\sim\pi_{\theta'}(\cdot|s)}\left[A_{\pi_\theta}(s, a) \right]
\end{align}
$$
因此只需要找到一个新策略 $\theta'$，使得 $\dfrac{1}{1-\gamma}\mathbb E_{s\sim\nu_{\pi_{\theta'}}}\mathbb E_{a\sim\pi_{\theta'}(\cdot|s)}\left[A_{\pi_\theta}(s, a) \right]\geq0\\$ ，就能保证策略的性能单调递增

> [!caution]
>
> 上式存在的问题是，要求解新策略 $\pi_{\theta'}$，但表达式里却又包含了 $\pi_{\theta'}$，即需要用所有新策略来收集数据，然后再取最优的
>
> 这样的做法显然不现实，而 TRPO 算法所做的就是对其进行近似处理

#### 近似方法

> 忽略新旧两个策略之间的状态访问分布的变化，直接用旧策略 $\pi_\theta$  的状态分布来代替
>
> 当新旧策略非常接近时，状态访问分布变化很小，这么近似是合理的

更改第一个期望的下标，使用旧策略的状态访问分布
$$
J(\theta')=J(\theta)+\dfrac1{1-\gamma}\mathbb E_{s\sim\nu_{\pi_{\theta}}}\mathbb E_{a\sim\pi_{\theta'}(\cdot|s)}\left[A_{\pi_\theta}(s, a) \right]
$$
由于动作仍然用新策略 $\pi_{\theta'}$ 采样得到，可以用重要性采样对动作分布进行处理，更改第二个期望的下标
$$
J(\theta')=J(\theta)+\dfrac1{1-\gamma}\mathbb E_{s\sim\nu_{\pi_{\theta}}}\mathbb E_{a\sim\pi_{\theta}(\cdot|s)}\left[\dfrac{\pi_{\theta'}(a|s)}{\pi_\theta(a|s)}A_{\pi_\theta}(s, a) \right]
$$
到这里，就可以用旧策略采样得到的数据来估计，并优化新策略了

##### KL 散度

> 为了保证新旧策略足够接近，使用 KL（库尔贝克-莱布勒（Kullback-Leibler，KL）） 散度来衡量新旧策略之间的距离

优化公式：
$$
\max_{\theta'}J(\theta')\\
s.t.\quad\mathbb E_{s\sim\nu_{\pi_{\theta_k}}}\left[D_{KL}(\pi_{\theta_k}(\cdot|s), \pi_{\theta'}(\cdot|s)\right]\leq\delta
$$

- 其中下标 $k$ 表示为第 $k$ 轮迭代

> [!important]
>
> KL 散度表达式根据概  率分布的类型（离散或连续）有所不同
>
> - **离散分布**
>
>   - 若有两个离散概率分布 $P$ 和$ Q$，则 $P$ 相对于 $Q$ 的 KL 散度为
>     $$
>     D_{KL}(P , Q) = \sum_{i=1}^{n} P(i) \log \frac{P(i)}{Q(i)}
>     $$
>
> - **连续分布**
>
>   - 若有两个连续概率密度函数 $p(x)$ 和 $q(x)$，则 $p$ 相对于 $q$ 的 KL 散度为
>     $$
>     D_{KL}(p, q) = \int_{-\infty}^{+\infty} p(x) \log \frac{p(x)}{q(x)} \, dx
>     $$

> [!note]
>
> 这里的不等式约束定义了策略空间中的一个 KL 球，被称为信任区域
>
> 在这个区域中，可以认为当前学习策略和环境交互的状态分布与上一轮策略最后采样的状态分布一致
>
> 进而可以基于一步行动的重要性采样方法，保证每次策略的梯度更新都能来带性能的提升

##### 泰勒近似

对目标函数 $J(\theta')$ 和 约束条件在 $\theta_k$​ 处进行泰勒展开，分别用一阶、二阶近似
$$
\dfrac1{1-\gamma}\mathbb E_{s\sim\nu_{\pi_{\theta_k}}}\mathbb E_{a\sim\pi_{\theta_k}(\cdot|s)}\left[\dfrac{\pi_{\theta'}(a|s)}{\pi_{\theta_k}(a|s)}A_{\pi_{\theta_k}}(s, a) \right]
\approx
\dfrac1{1-\gamma}g^T(\theta'-\theta_k)\\
\mathbb E_{s\sim\nu_{\pi_{\theta_k}}}\left[D_{KL}(\pi_{\theta_k}(\cdot|s), \pi_{\theta'}(\cdot|s)\right]
\approx
\dfrac12(\theta'-\theta_k)^TH(\theta'-\theta_k)
$$

- 其中
  - $g=\nabla_{\theta'}\mathbb E_{s\sim\nu_{\pi_{\theta_k}}}\mathbb E_{a\sim\pi_{\theta_k}(\cdot|s)}\left[\dfrac{\pi_{\theta'}(a|s)}{\pi_{\theta_k}(a|s)}A_{\pi_{\theta_k}}(s, a) \right]$ ，表示目标函数的梯度
  - $H=\mathbb H\left[E_{s\sim\nu_{\pi_{\theta_k}}}\left[D_{KL}(\pi_{\theta_k}(\cdot|s), \pi_{\theta'}(\cdot|s)\right]\right]$ ，表示新旧策略之间的平均 KL 距离的海森矩阵

> [!note]
>
> 目标函数光滑且在 $\theta_k$ 附近变化平缓，用一阶泰勒近似足够
>
> 约束的 KL 散度在 $\theta_k$ 处梯度为 $0$，一阶项消失，必须用二阶泰勒近似
>
> ==**矩阵导数定义**==
>
> 假设有一个多元函数 $f(\theta)$，其中 $\theta = [\theta_1, \theta_2, ..., \theta_n]^T$（如策略网络的权重向量）
>
> - 首先计算函数对每个参数的一阶偏导数，组成「梯度向量」$g = \nabla f(\theta) = [\frac{\partial f}{\partial \theta_1}, \frac{\partial f}{\partial \theta_2}, ..., \frac{\partial f}{\partial \theta_n}]^T$
>
> - 再计算每个一阶偏导数对所有参数的「二阶偏导数」，组成一个 $n \times n$ 的对称矩阵，即为海森矩阵 $H$
>   $$
>   H = \nabla^2 f(\theta) = \begin{bmatrix} \frac{\partial^2 f}{\partial \theta_1^2} & \frac{\partial^2 f}{\partial \theta_1 \partial \theta_2} & ... & \frac{\partial^2 f}{\partial \theta_1 \partial \theta_n} \\ \frac{\partial^2 f}{\partial \theta_2 \partial \theta_1} & \frac{\partial^2 f}{\partial \theta_2^2} & ... & \frac{\partial^2 f}{\partial \theta_2 \partial \theta_n} \\ ... & ... & ... & ... \\ \frac{\partial^2 f}{\partial \theta_n \partial \theta_1} & \frac{\partial^2 f}{\partial \theta_n \partial \theta_2} & ... & \frac{\partial^2 f}{\partial \theta_n^2} \end{bmatrix}
>   $$
>
> 梯度 $g$ 告诉我们函数在该点的变化方向（往哪个方向走函数值变大 / 变小）
>
> 海森矩阵 $H$ 告诉我们函数在该方向的变化速率有多快（曲率）
>
> - 比如 H 正定，函数在该点是凸的，梯度下降能找到最小值
> - H 负定则是凹的
>
> ==**多元函数的泰勒展开**==
>
> 泰勒展开的本质是：**用多项式近似一个光滑函数在某点附近的行为**（离该点越近，近似越准确）
>
> 这正是 TRPO 需要的
>
> - 因为 TRPO 假设新旧策略差异小（信任区域）
> - 即 $\theta'$ 离 $\theta_k$ 很近，适合用泰勒展开简化计算
>
> **一元函数的泰勒展开**
>
> 回顾一元函数 $f(x)$ 在 $x=a$ 处的泰勒展开
>
> - 一阶近似（线性近似）：只保留常数项+一阶导数项，适合函数变化平缓的情况
>   $$
>   f(x) \approx f(a) + f'(a)(x - a)
>   $$
>
> - 二阶近似（二次近似）：多了二阶导数项，能捕捉函数的弯曲，近似更精准
>   $$
>   f(x) \approx f(a) + f'(a)(x - a) + \frac{1}{2}f''(a)(x - a)^2
>   $$
>
> **多元函数的泰勒展开**
>
> 对于多元函数 $f(\theta)$（$\theta$ 是 n 维向量），在 $\theta = \theta_k$ 处的泰勒展开
>
> - 一阶近似（线性近似）
>   $$
>   f(\theta') \approx f(\theta_k) + g_k^T (\theta' - \theta_k)
>   $$
>   其中
>
>   -  $g_k = \nabla f(\theta_k)$ 是 $\theta_k$ 处的梯度向量
>   - $(\theta' - \theta_k)$ 是参数增量向量
>   - $g_k^T (\theta' - \theta_k)$ 是向量内积
>     - 对应一元函数的 $f'(a)(x-a)$
>
> 
>
> - 二阶近似（二次近似）
>   $$
>   f(\theta') \approx f(\theta_k) + g_k^T (\theta' - \theta_k) + \frac{1}{2} (\theta' - \theta_k)^T H_k (\theta' - \theta_k)
>   $$
>   其中
>
>   -  $H_k = \nabla^2 f(\theta_k)$ 是 $\theta_k$ 处的海森矩阵
>   - $(\theta' - \theta_k)^T H_k (\theta' - \theta_k)$ 是二次型
>     - 对应一元函数的 $\frac{1}{2}f''(a)(x-a)^2$
>
> ==**TRPO 中的泰勒展开**==
>
> **TRPO 的目标是快速找到能提升性能的新策略**
>
> 目标函数一阶近似：
>
> - 函数值项与 $\theta'$ 无关，为常数项，不能提供明确方向
> - 一阶近似能提供足够的优化方向，且计算成本低
> - 二阶项计算复杂（n 维参数的海森矩阵是 $n \times n$，算力开销大）
> - 结合信任区域内策略变化小的假设，一阶近似的误差足够小，完全满足需求
>
> 约束函数二阶近似
>
> - 常数项为 $0$ （相同分布的 KL 散度为 $0$）
> - 一阶近似几乎为 $0$
> - 故需要计算二阶项来近似 KL 散度在 $\theta_k$ 附近的行为
>   - 海森矩阵 $H$ 描述了 KL 散度的曲率，即新策略偏离旧策略时，KL 散度增长的速率
>   - 这正是 TRPO 需要的：约束 KL 散度的增长不超过 $\delta$（信任区域大小）

经过近似，优化目标变成
$$
\theta_{k+1}=\arg\max_{\theta'}\left[g^T(\theta'-\theta_k)\right]\\
s.t.\quad \dfrac12(\theta'-\theta_k)^TH(\theta'-\theta_k)\leq\delta
$$
此时，即可用**卡罗需-库恩-塔克**（Karush-Kuhn-Tucker，KKT）条件直接导出上述问题的解
$$
\theta_{k+1}=\theta_k+\sqrt{\dfrac{2\delta}{g^TH^{-1}g}}H^{-1}g
$$

**几何解释**

- **方向**：更新方向是自然梯度方向 $H^{-1} g$
- **步长**：自适应步长 $\sqrt{\frac{2\delta}{g^T H^{-1} g}}\\$，确保在信任域内
- **信任域**：约束定义了一个以 $\theta_k$ 为中心的椭球，形状由 $H$ 决定

> [!note]
>
> **Karush-Kuhn-Tucker（KKT）条件是非线性规划（nonlinear programming）最佳解的必要条件**
>
> KKT 条件将 Lagrange 乘数法（Lagrange multipliers）所处理涉及等式的约束优化问题推广至不等式
>
> 在实际应用上，KKT条件一般不存在代数解，许多优化算法可供数值计算选用
>
> ==**等式约束优化问题**==
>
> Lagrange 乘数法是等式约束优化问题的典型解法
>
> 给定一个目标函数 $ f:\mathbb{R}^n\to\mathbb{R} $，我们希望找到 $ \mathbf{x}\in\mathbb{R}^n $，在满足约束条件 $ g(\mathbf{x})=0 $ 的前提下，使得 $ f(\mathbf{x}) $ 有最小值。这个约束优化问题记为
>
> $$
> \begin{array}{ll}
> \hbox{min}&f(\mathbf{x})\\
> \hbox{s.t.}&g(\mathbf{x})=0
> \end{array}
> $$
>
> - 为方便分析，假设 $ f $ 与 $ g $ 是连续可导函数
>
> 定义 Lagrangian 函数
> $$
> L(\mathbf{x},\lambda)=f(\mathbf{x})+\lambda g(\mathbf{x})
> $$
>
> - 其中 $ \lambda $ 称为 Lagrange 乘数
>
> Lagrange 乘数法将原本的约束优化问题转换成等价的无约束优化问题
> $$
> \underset{\mathbf{x},\lambda}{\hbox{min}}~~L(\mathbf{x},\lambda)
> $$
> 计算 $ L $ 对 $ \mathbf{x} $ 与 $ \lambda $ 的偏导数并设为零，可得最优解的必要条件
> $$
> \begin{aligned}
> \nabla_{\mathbf{x}}L&=\frac{\partial L}{\partial\mathbf{x}}=\nabla f+\lambda\nabla g=\mathbf{0}\\
> \nabla_{\lambda}L&=\frac{\partial L}{\partial\lambda}=g(\mathbf{x})=0
> \end{aligned}
> $$
>
> - 其中第一式为梯度条件，第二式为约束条件
>
> 解开上面 $ n+1 $ 个方程式可得 $ L(\mathbf{x},\lambda) $ 的 驻点 $ \mathbf{x}^\star $ 以及 $ \lambda $ 的值（可正可负）
>
> ==**不等式约束优化问题**==
>
> 接下来将约束等式 $ g(\mathbf{x})=0 $ 推广为不等式 $ g(\mathbf{x})\le 0 $
>
> $$
> \begin{array}{ll}
> \hbox{min}&f(\mathbf{x})\\
> \hbox{s.t.}&g(\mathbf{x})\le 0.
> \end{array}
> $$
>
> - 约束不等式 $ g(\mathbf{x})\le 0 $ 称为原始可行性（primal feasibility）
>
> 据此我们定义可行域（feasible region）
> $$
>  K=\{\mathbf{x}\in\mathbb{R}^n\mid g(\mathbf{x})\le 0\} 
> $$
> 假设 $ \mathbf{x}^\star $ 为满足约束条件的最佳解，分开两种情况讨论
>
> 1. $ g(\mathbf{x}^\star)<0 $，最佳解位于 $ K $ 的内部，称为内部解（interior solution），这时约束条件是无效的
> 2. $ g(\mathbf{x}^\star)=0 $，最佳解落在 $ K $ 的边界，称为边界解（boundary solution），此时约束条件是有效的
>
> 这两种情况的最佳解具有不同的必要条件
>
> 1. **内部解**
>    - 在约束条件无效的情形下，$ g(\mathbf{x}) $ 不起作用
>    - 约束优化问题退化为无约束优化问题，因此驻点 $ \mathbf{x}^\star $ 满足 $ \nabla f =\mathbf{0} $ 且 $ \lambda=0 $
> 2. **边界解**
>    - 在约束条件有效的情形下，约束不等式变成等式 $ g(\mathbf{x})=0 $，这与前述 Lagrange 乘数法的情况相同
>    - 可以证明驻点 $ \mathbf{x}^\star $ 发生于 $ \nabla f\in\hbox{span}\{\nabla g\} $
>      - 即存在 $ \lambda $ 使得 $ \nabla f=-\lambda\nabla g $，但这里 $ \lambda $ 的正负号是有其意义的
>      - 因为我们希望最小化  $ f $
>        - 梯度 $ \nabla f $（函数 $ f $在点 $ \mathbf{x} $ 的最陡上升方向）应该指向可行域 $ K $ 的内部
>          - 因为最优解最小值是在边界取得的
>
>        - 但 $ \nabla g $ 指向 $ K $ 的外部（即 $ g(\mathbf{x})>0 $ 的区域，因为约束是小于等于0）
>
>      - 因此 $ \lambda\ge 0 $，称为对偶可行性（dual feasibility）
>
>
> 因此，不论是内部解或边界解，$ \lambda g(\mathbf{x})=0 $ 恒成立，称为互补松弛性（complementary slackness）
>
> 整合上述两种情况
>
> - 最佳解的必要条件包括 Lagrangian 函数 $ L(\mathbf{x},\lambda) $ 的梯度条件、原始可行性、对偶可行性，以及互补松弛性
>
> $$
> \begin{aligned}
> \nabla_{\mathbf{x}}L&=\nabla f+\lambda\nabla g=\mathbf{0}\\
> g(\mathbf{x})&\le 0\\
> \lambda& \ge 0\\
> \lambda g(\mathbf{x})&=0.
> \end{aligned}
> $$
>
> 这些条件合称为 Karush-Kuhn-Tucker (KKT) 条件
>
> - 如果我们要最大化 $ f(\mathbf{x}) $ 且受限于 $ g(\mathbf{x})\le 0 $，那么对偶可行性要改成 $ \lambda\le 0 $
>
> > [!tip]
> >
> > 上面结果可推广至多个约束等式与约束不等式的情况
> >
> > 考虑标准约束优化问题(或称非线性规划)：
> > $$
> > \begin{array}{lll}
> > \hbox{min}&f(\mathbf{x})\\
> > \hbox{s.t.}&g_j(\mathbf{x})=0,&j=1,\ldots,m,\\
> > &h_k(\mathbf{x})\le 0,&k=1,\ldots,p.
> > \end{array}
> > $$
> > 定义 Lagrangian 函数
> >
> > $$
> > L\left(\mathbf{x},\{\lambda_j\},\{\mu_k\}\right)=f(\mathbf{x})+\sum_{j=1}^m\lambda_jg_j( \mathbf{x})+\sum_{k=1}^p\mu_kh_k(\mathbf{x})
> > $$
> > 其中
> >
> > -  $ \lambda_j $ 是对应 $ g_j(\mathbf{x})=0 $ 的 Lagrange 乘数
> > - $ \mu_k $ 是对应 $ h_k(\mathbf{x})\le 0 $ 的 Lagrange 乘数（或称 KKT 乘数）
> >
> > KKT条件包括
> > $$
> > \begin{aligned}
> > \nabla_{\mathbf{x}}L&=\mathbf{0}\\
> > g_j(\mathbf{x})&=0,~~j=1,\ldots,m,\\
> > h_k(\mathbf{x})&\le 0,\\
> > \mu_k&\ge 0,\\
> > \mu_k h_k(\mathbf{x})&=0,~~k=1,\ldots,p.
> > \end{aligned}
> > $$
>
> > [!caution]
> >
> > 在使用KKT条件时需要满足约束条件是仿射函数

==求解 TRPO 中的优化目标==

带约束的优化问题
$$
\begin{aligned}
\max_{\theta'} &\quad g^T(\theta' - \theta_k) \\
\text{s.t.} &\quad \frac{1}{2}(\theta' - \theta_k)^T H (\theta' - \theta_k) \leq \delta
\end{aligned}
$$
**KKT 条件推导**

1. 构造拉格朗日函数
   - 引入拉格朗日乘子 $\lambda \geq 0$，构造拉格朗日函数


$$
\mathcal{L}(\theta', \lambda) = g^T(\theta' - \theta_k) - \lambda \left[ \frac{1}{2}(\theta' - \theta_k)^T H (\theta' - \theta_k) - \delta \right]
$$

2. 梯度条件（Stationarity）

   - 对 $\theta'$ 求梯度并令其为 $0$
     $$
     \nabla_{\theta'} \mathcal{L} = g - \lambda H (\theta' - \theta_k) = 0
     $$

   - 由此得到
     $$
     \lambda H (\theta' - \theta_k) = g
     $$

   - 由于 $H$ 对称正定可逆，解得
     $$
     \theta' - \theta_k = \frac{1}{\lambda} H^{-1} g \quad \tag{1}
     $$

     - 由于 $g \neq 0$ 且 $H^{-1}$ 正定，$H^{-1}g \neq 0$，因此

       - 如果 $\lambda = 0$，则 $\theta' - \theta_k$ 会趋于无穷大，违反约束

       - 所以必须有 $\lambda > 0$

3. 原始可行性（Primal Feasibility）

$$
\frac{1}{2}(\theta' - \theta_k)^T H (\theta' - \theta_k) \leq \delta
$$

4. 对偶可行性（Dual Feasibility）

$$
\lambda \geq 0
$$

5. 互补松弛条件（Complementary Slackness）

$$
\lambda \left[ \frac{1}{2}(\theta' - \theta_k)^T H (\theta' - \theta_k) - \delta \right] = 0
$$

**求解过程**

1. 由于 $\lambda > 0$，根据互补松弛条件，约束必须是在边界上的，即
   $$
   \frac{1}{2}(\theta' - \theta_k)^T H (\theta' - \theta_k) = \delta
   $$

2. 将公式 (1) 代入步骤 1 的约束条件
   $$
   \frac{1}{2} \left( \frac{1}{\lambda} H^{-1} g \right)^T H \left( \frac{1}{\lambda} H^{-1} g \right) = \delta
   $$

   - 简化
     $$
     \frac{1}{2\lambda^2} g^T H^{-1} H H^{-1} g = \delta
     $$

     $$
     \frac{1}{2\lambda^2} g^T H^{-1} g = \delta
     $$

3. 解出 $\lambda$
   $$
   \lambda^2 = \frac{g^T H^{-1} g}{2\delta}
   $$

   - 由于 $\lambda > 0$，取正根
     $$
     \lambda = \sqrt{\frac{g^T H^{-1} g}{2\delta}} \quad \tag{2}
     $$
     

4. 将公式 (2) 代回公式 (1)
   $$
   \theta' - \theta_k = \frac{1}{\lambda} H^{-1} g = \frac{1}{\sqrt{\frac{g^T H^{-1} g}{2\delta}}} H^{-1} g
   $$

   - 整理得到
     $$
     \theta' - \theta_k = \sqrt{\frac{2\delta}{g^T H^{-1} g}} H^{-1} g
     $$

5. 求出结果

$$
\theta_{k+1} = \theta_k + \sqrt{\frac{2\delta}{g^T H^{-1} g}} H^{-1} g
$$

##### 共轭梯度

> 虽然我们得到了更新表达式，但是一般来说，用神经网络表示的策略函数的参数数量都是成千上万的，计算和存储海森矩阵的逆矩阵会耗费大量的内存资源和时间

因此==引入共轭梯度法（conjugate gradient method）==

1. 首先改写成使用 步长$\times$方向 的方式来更新
   $$
   \theta_{k+1}=\theta_k+\beta x
   $$
   其中 $x$ 即为参数的更新方向
   $$
   x=H^{-1}g
   $$

2. 推导步长（根据互补松弛条件）
   $$
   \frac{1}{2}(\beta x)^T H (\beta x) = \delta\implies\beta=\sqrt{\dfrac{2\delta}{x^THx}}
   $$

3. 因此，问题转化为使用共轭梯度法来求解 $Hx=g$，而不需要求逆矩阵

> [!note]
>
> 共轭梯度法是专门针对**对称正定矩阵**（如海森矩阵 H）的线性方程组 $Hx = g$ 的迭代算法
>
> 核心优势是**无需存储整个矩阵，只需迭代更新向量**
>
> 在理论上能在 “矩阵维度次数” 内收敛（实际中因数值误差会提前停止）
>
> ==共轭梯度法的流程==
>
> 1. 初始化
>
>    - $x_0 = 0$（初始解向量，假设从 $0$ 开始迭代）
>    - $r_0 = g - Hx_0 = g$（初始残差，衡量当前解与真实解的差距）
>    - $p_0 = r_0$（初始搜索方向，沿残差方向搜索）
>
> 2. 迭代更新：对 $k = 0, 1, ..., N$
>
>    - 计算步长 $\alpha_k$
>      $$
>      \alpha_k = \frac{r_k^T r_k}{p_k^T H p_k}
>      $$
>
>      - 在搜索方向 $p_k$ 上走多远，能让残差下降最快
>      - 分子是残差的能量，分母是搜索方向的曲率
>
>    - 更新解向量 $x_{k+1}$
>      $$
>      x_{k+1} = x_k + \alpha_k p_k
>      $$
>
>      - 沿搜索方向 $p_k$ 走 $\alpha_k$ 步，得到新的近似解
>
>    - 更新残差 $r_{k+1}$
>      $$
>      r_{k+1} = r_k - \alpha_k H p_k
>      $$
>
>      - 计算新的残差，衡量更新后解的误差
>
>    - ==提前终止条件==
>
>      - 若 $r_{k+1}^T r_{k+1}$ 非常小（残差几乎为 0），说明解足够精确，停止迭代
>
>    - 计算方向更新系数  $\beta_k$
>      $$
>      \beta_k = \frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}
>      $$
>
>      - 调整下一轮的搜索方向，保证 共轭性（不同搜索方向在 H 下正交，避免重复搜索）
>
>    - 更新搜索方向  $p_{k+1}$
>      $$
>      p_{k+1} = r_{k+1} + \beta_k p_k
>      $$
>
>      - 结合新残差和历史方向，生成下一个共轭搜索方向

##### 矩阵 - 向量乘积的梯度表示

> 共轭梯度法中需要计算 $H p_k$（海森矩阵与搜索方向的乘积），但直接计算 $H$ 会面临问题
>
> - $H$ 是高维矩阵，存储和计算量极大
> - 显式计算 $H$ 容易引入数值误差，且对内存不友好
>
> 因此，==只计算 H 与向量的乘积 $Hv$==，$v$ 是任意向量，而不显式构造 $H$ 矩阵

1. 对于 KL 散度
   $$
   D_{KL}\left(\pi_{\theta_k}(\cdot|s), \pi_{\theta'}(\cdot|s)\right)
   $$

   - 其关于 $\theta'$ 的一阶导数为 $\nabla_{\theta'} D_{KL}$
   - 二阶导数是海森矩阵  $H=\nabla_{\theta'}^2 D_{KL}$

2. 计算 $Hv$，可以利用 梯度的梯度推导
   $$
   Hv =\nabla_{\theta} \left( \left( \nabla_{\theta} D_{KL} \right)^T \right)v= \nabla_{\theta} \left( \left( \nabla_{\theta} D_{KL} \right)^T v \right)
   $$

   - 即先用梯度和向量点乘后，再计算梯度

##### 线性搜索

> 经过大量近似，可能不能保证新的策略更优了（超出约束）
>
> 所以要用一个线性参数，来简单地缩放步长，使得新策略确实满足约束条件

线性搜索的目标是找到最小的非负整数 $i$

- 使得按以下公式更新的策略 $\theta_{k+1}$​ 同时满足约束条件
  $$
  \theta_{k+1} = \theta_k + \alpha^i \sqrt{\frac{2\delta}{x^T H x}} \, x
  $$
  其中

  - 步长缩放因子 $\alpha \in (0,1)$
    - 控制每次搜索的步长衰减，比如$\alpha=0.5$，则每次尝试的步长是上一次的一半

  - 迭代次数 $i$
    - 从 0 开始，依次尝试$\alpha^0=1$、$\alpha^1=\alpha$、$\alpha^2=\alpha^2、\ldots$， 直到找到满足条件的步长

> [!important]
>
> 总结流程
>
> 1. 获取当前策略 $\pi_\theta$ 的采样轨迹
> 2. 根据收集到的数据和价值网络，估计每个状态动作对的优势 $A(s_t,a_t)$
> 3. 计算策略目标函数的梯度 $g$
> 4. 用共轭梯度法计算 $x=H^{-1}g$
> 5. 线性搜索找到 $i$，并更新策略   $\theta_{k+1} = \theta_k + \alpha^i \sqrt{\frac{2\delta}{x^T H x}} \, x\\$
> 6. 更新价值网络参数（同 Actor Critic）

#### 广义优势估计 GAE

> **广义优势估计**（Generalized Advantage Estimation，GAE）是一种估计 $A(s_t,a_t)$ 的方法，用于上述流程的第二步

单步 TD-error （偏差小、方差大）
$$
A_t=Q_t-V_t=\delta_t=r_t+\gamma v(s_{t+1})-v(s_t)
$$
$n$ 步 TD-error （ $n$ 越大，偏差越大，方差越小）
$$
A_t(n)=\delta_t+\gamma\delta_{t+1}+\cdots+\gamma^{n-1}\delta_{t+n-1}=\sum_{l=0}^{n-1}\gamma^l\delta_{t+l}
$$
广义优势估计 GAE（将不同步数的优势估计进行指数加权平均，实现偏差与方差的平衡）
$$
A_t^{GAE}=\sum_{l=0}^{\infty}(\gamma\lambda)^l\delta_{t+l}
$$

- 其中 $\lambda\in[0,1]$ 是一个超参数，$\lambda=0$ 时，变成单步 TD-error，$\lambda=1$ 时，变成整段轨迹步长的 $n$ 步 TD-error
  - 一般取 $\lambda=0.95$

```python
def compute_advantage(gamma, lmbda, td_delta):
    td_delta = td_delta.detach().numpy()
    advantage_list = []
    advantage = 0.0
    for delta in td_delta[::-1]:
        advantage = gamma * lmbda * advantage + delta
        advantage_list.append(advantage)
    advantage_list.reverse()
    return torch.tensor(advantage_list, dtype=torch.float)
```

#### 主要代码

创建 `Actor` 和 `Critic` （同 Actor Critic 算法）

```python
class PolicyNet(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(PolicyNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return F.softmax(self.fc2(x), dim=1)


class ValueNet(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim):
        super(ValueNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return self.fc2(x)
```

创建 `Agent`

```python
class TRPO:
    def __init__(self, hidden_dim, state_space, action_space, lmbda,
                 kl_constraint, alpha, critic_lr, gamma, device):
        state_dim = state_space.shape[0]
        action_dim = action_space.n
        # 策略网络参数不需要优化器更新
        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)
        self.critic = ValueNet(state_dim, hidden_dim).to(device)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)
        
        self.gamma = gamma
        # GAE参数
        self.lmbda = lmbda  
        # KL距离最大限制
        self.kl_constraint = kl_constraint  
        # 线性搜索参数
        self.alpha = alpha  
        self.device = device

    def take_action(self, state):
        state = torch.tensor([state], dtype=torch.float).to(self.device)
        probs = self.actor(state)
        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()
        return action.item()
	
    # 计算黑塞矩阵和一个向量的乘积
    def hessian_matrix_vector_product(self, states, old_action_dists, vector):
        new_action_dists = torch.distributions.Categorical(self.actor(states))
        # 计算平均KL距离
        kl = torch.mean(
            	torch.distributions.kl.kl_divergence(old_action_dists, new_action_dists)
        	 ) 
        kl_grad = torch.autograd.grad(kl, self.actor.parameters(), create_graph=True)
        kl_grad_vector = torch.cat([grad.view(-1) for grad in kl_grad])
        # KL距离的梯度先和向量进行点积运算
        kl_grad_vector_product = torch.dot(kl_grad_vector, vector)
        grad2 = torch.autograd.grad(kl_grad_vector_product, elf.actor.parameters())
        grad2_vector = torch.cat([grad.view(-1) for grad in grad2])
        return grad2_vector
	
    # 共轭梯度法求解方程
    def conjugate_gradient(self, grad, states, old_action_dists):  
        x = torch.zeros_like(grad)
        r = grad.clone()
        p = grad.clone()
        rdotr = torch.dot(r, r)
        # 共轭梯度主循环
        for i in range(10):  
            Hp = self.hessian_matrix_vector_product(states, old_action_dists, p)
            alpha = rdotr / torch.dot(p, Hp)
            x += alpha * p
            r -= alpha * Hp
            new_rdotr = torch.dot(r, r)
            if new_rdotr < 1e-10:
                break
            beta = new_rdotr / rdotr
            p = r + beta * p
            rdotr = new_rdotr
        return x
    
 	# 计算策略目标
    def compute_surrogate_obj(self, states, actions, advantage, old_log_probs, actor): 
        log_probs = torch.log(actor(states).gather(1, actions))
        ratio = torch.exp(log_probs - old_log_probs)
        return torch.mean(ratio * advantage)
	
    # 线性搜索
    def line_search(self, states, actions, advantage, 
                    old_log_probs, old_action_dists, max_vec):  
        old_para = torch.nn.utils.convert_parameters.parameters_to_vector(
            			self.actor.parameters()
        			)
        old_obj = self.compute_surrogate_obj(
            		states, actions, advantage,
                 	old_log_probs, self.actor
        		  )
        # 线性搜索主循环
        for i in range(15):  
            coef = self.alpha**i
            new_para = old_para + coef * max_vec
            new_actor = copy.deepcopy(self.actor)
            torch.nn.utils.convert_parameters.vector_to_parameters(
                new_para, 
                new_actor.parameters()
            )
            new_action_dists = torch.distributions.Categorical(new_actor(states))
            kl_div = torch.mean(
                 torch.distributions.kl.kl_divergence(old_action_dists, new_action_dists)
            	)
            new_obj = self.compute_surrogate_obj(
                		states, actions, advantage,
                        old_log_probs, new_actor
            		  )
            if new_obj > old_obj and kl_div < self.kl_constraint:
                return new_para
        return old_para
    
	# 更新策略函数
    def policy_learn(self, states, actions, old_action_dists, old_log_probs, advantage):  
        surrogate_obj = self.compute_surrogate_obj(
            				states, actions, advantage,
                            old_log_probs, self.actor
        				)
        grads = torch.autograd.grad(surrogate_obj, self.actor.parameters())
        obj_grad = torch.cat([grad.view(-1) for grad in grads]).detach()
        # 用共轭梯度法计算x = H^(-1)g
        descent_direction = self.conjugate_gradient(obj_grad, states, old_action_dists)

        Hd = self.hessian_matrix_vector_product(states,old_action_dists,descent_direction)
        max_coef = torch.sqrt(
            			2 * self.kl_constraint 
                        / (torch.dot(descent_direction, Hd) + 1e-8)
 					)
        # 线性搜索
        new_para = self.line_search(
            			states, actions, advantage, old_log_probs,
                        old_action_dists, descent_direction * max_coef
                   )  
        # 用线性搜索后的参数更新策略
        torch.nn.utils.convert_parameters.vector_to_parameters(
            new_para, 
            self.actor.parameters()
        )  

    def update(self, transition_dict):
        states = torch.tensor(transition_dict['states'],
                              dtype=torch.float).to(self.device)
        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(self.device)
        rewards = torch.tensor(transition_dict['rewards'],
                               dtype=torch.float).view(-1, 1).to(self.device)
        next_states = torch.tensor(transition_dict['next_states'],
                                   dtype=torch.float).to(self.device)
        dones = torch.tensor(transition_dict['dones'],
                             dtype=torch.float).view(-1, 1).to(self.device)
        td_target = rewards + self.gamma * self.critic(next_states) * (1 - dones)
        td_delta = td_target - self.critic(states)
        advantage = compute_advantage(
            			self.gamma, self.lmbda,
                        td_delta.cpu()
        			).to(self.device)
        old_log_probs = torch.log(self.actor(states).gather(1, actions)).detach()
        old_action_dists = torch.distributions.Categorical(self.actor(states).detach())
        critic_loss = torch.mean(F.mse_loss(self.critic(states), td_target.detach()))
        # 更新价值函数
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()  
        # 更新策略函数
        self.policy_learn(states, actions, old_action_dists, old_log_probs, advantage)
```



### :five: PPO 

> PPO（Proximal Policy Optimization，近端策略优化）的优化目标与 TRPO 相同
>
> 但 PPO 用了一些相对简单的方法来求解
>
> :star: 大量的实验结果表明，与 TRPO 相比，PPO 能学习得一样好（甚至更快）

> [!tip]
>
> 以下两种方法中，选取一种即可，一般将其代表的损失称为 `surrogate loss`

#### PPO-Penalty

PPO-Penalty 用拉格朗日乘数法，直接将 KL 散度的限制放进了目标函数中，使其变成无约束的优化问题
$$
\arg\max_\theta\mathbb E_{s\sim\nu_{\pi_{\theta}}}\mathbb E_{a\sim\pi_{\theta}(\cdot|s)}\left[\dfrac{\pi_{\theta'}(a|s)}{\pi_\theta(a|s)}A_{\pi_\theta}(s, a)-\beta D_{KL}[\pi_{\theta'}(\cdot|s),\pi_\theta(\cdot|s)] \right]
$$

- 其中 $\beta$ 为在迭代过程中不断更新的系数

$\beta$ 更新规则

- 令 $d_k=D_{KL}[\pi_{\theta'}(\cdot|s),\pi_\theta(\cdot|s)]$
  - 如果 $d_k<\dfrac{\delta}{1.5}$，则 $\beta_{k+1}=\dfrac{\beta_k}2$
  - 如果 $d_k>\dfrac{\delta}{1.5}$，则 $\beta_{k+1}=2{\beta_k}$
  - 否则 $\beta_{k+1}=\beta_k$

> [!note]
>
> $\delta$ 是提前设好的超参数，用于限制学习策略和之前一轮策略的差距

#### PPO-Clip

PPO-Clip 在目标函数中直接限制新旧策略的差距不会太大
$$
\arg\max_\theta\mathbb E_{s\sim\nu_{\pi_{\theta}}}\mathbb E_{a\sim\pi_{\theta}(\cdot|s)}\left[\min\bigg(\dfrac{\pi_{\theta'}(a|s)}{\pi_\theta(a|s)}A_{\pi_\theta}(s, a),\text{clip}\big(\dfrac{\pi_{\theta'}(a|s)}{\pi_\theta(a|s)}, 1-\epsilon, 1+\epsilon\big)A_{\pi_\theta}(s, a)\bigg) \right]
$$

> [!note]
>
> $\epsilon$ 是提前设好的超参数，表示截断范围，在重要性采样上限制新旧策略的差距

### 
